{"cells":[{"cell_type":"markdown","metadata":{"id":"-Qk7-5P4Rrc-"},"source":["# Deep Q-network Practice"]},{"cell_type":"markdown","metadata":{"id":"fEVv-3VNR1zc"},"source":["If you run in jupyter, turn \n","\n","```\n","colab = False\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"usZPA_lZYSsI"},"outputs":[],"source":["# colab = True"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29244,"status":"ok","timestamp":1655363957020,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"Y9VBDokpR4we","outputId":"01284c0e-3fe2-4681-e07a-93e37fbf06b9"},"outputs":[],"source":["# if colab:\n","#     !pip install gym pyvirtualdisplay > /dev/null 2>&1\n","#     !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","#     !apt-get update > /dev/null 2>&1\n","#     !apt-get install cmake > /dev/null 2>&1\n","#     !pip install --upgrade setuptools 2>&1\n","#     !pip install ez_setup > /dev/null 2>&1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16869,"status":"ok","timestamp":1655363975577,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"PsyShng6RrdA","outputId":"a6a7707c-fdf0-403e-b7a9-9429bbfa12b3"},"outputs":[],"source":["# if colab:\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","\n","#     %cd /content/drive/MyDrive/Colab\\ Notebooks/drlcourse-main/day2/dqn\n","#     !ls"]},{"cell_type":"markdown","metadata":{"id":"WELkYeNcRrdD"},"source":["# 0. Define Q-network & policy-network"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4388,"status":"ok","timestamp":1655363983210,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"USpBW4V1RrdE"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import MSELoss\n","import torch.nn.functional as F\n","import copy\n","import os\n","import csv\n","import numpy as np\n","import torch\n","from torch.optim import Adam\n","from buffer import ReplayBuffer\n","from utils import save_snapshot, recover_snapshot, load_model\n","from schedule import LinearSchedule\n","import gym"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1655363984572,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"bxWmxxWIRrdF","outputId":"b4eceed4-dd9c-4176-f9e1-ad620fbe066e"},"outputs":[{"name":"stdout","output_type":"stream","text":["current device = cuda\n"]}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('current device =', device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1655363986248,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"wKSTslpORrdG"},"outputs":[],"source":["# critic network definition\n","# multi-layer perceptron (with 2 hidden layers)\n","class Critic(nn.Module):\n","    def __init__(self, state_dim, num_action):\n","        super(Critic, self).__init__()\n","        # [TODO] Design newtork!\n","        self.fc1 = nn.Linear(state_dim, 256)\n","        self.fc2 = nn.Linear(256, 256)\n","        self.fc3 = nn.Linear(256, num_action)\n","\n","\n","    def forward(self, state):\n","        # given a state s, the network returns a vector Q(s,) of length |A|\n","        # [TODO] Design network!\n","        # cf) F.tanh, F.sigmoid https://pytorch.org/docs/stable/nn.functional.html \n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","        q = self.fc3(x)\n","        return q"]},{"cell_type":"markdown","metadata":{"id":"QAr9QxHNRrdG"},"source":["# 1. Define DQN agent"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":319,"status":"ok","timestamp":1655363987921,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"MGYt9AN1RrdH"},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, obs_dim, num_act):\n","        self.obs_dim = obs_dim\n","        self.num_act = num_act\n","        # networks\n","        self.critic = Critic(obs_dim, num_act).to(device)\n","                \n","    def act(self, state, epsilon=0.0):\n","        # simple implementation of \\epsilon-greedy method\n","        # TODO : Complete epsilon-greedy action selection\n","        # Hint : np.randon.rand() will generate random number in [0,1]\n","        if np.random.rand() < epsilon:\n","            return np.random.randint(self.num_act)\n","        else:\n","            # greedy selection\n","            self.critic.eval() # Evaluation mode\n","            s = torch.Tensor(state).view(1, self.obs_dim).to(device)\n","            q = self.critic(s)\n","            return np.argmax(q.cpu().detach().numpy())"]},{"cell_type":"markdown","metadata":{"id":"fxyKOc9aRrdH"},"source":["# 2. Implement one-step param update"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1655363990930,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"wkRLMO0yRrdI"},"outputs":[],"source":["def update(agent, replay_buf, gamma, critic_optim, target_critic, tau, batch_size):\n","    # agent : agent with networks to be trained\n","    # replay_buf : replay buf from which we sample a batch\n","    # actor_optim / critic_optim : torch optimizers\n","    # tau : parameter for soft target update\n","    \n","    # Train mode\n","    agent.critic.train()\n","\n","    # Sample mini-batch from replay buffer\n","    batch = replay_buf.sample_batch(batch_size)\n","\n","    # unroll batch\n","    # no_grad since target value!\n","    with torch.no_grad():\n","        observations = torch.Tensor(batch['state']).to(device)\n","        actions = torch.tensor(batch['action'], dtype=torch.long).to(device)\n","        rewards = torch.Tensor(batch['reward']).to(device)\n","        next_observations = torch.Tensor(batch['next_state']).to(device)\n","        terminals = torch.Tensor(batch['done']).to(device)\n","        mask = 1.0 - terminals\n","\n","        next_q = torch.unsqueeze(target_critic(next_observations).max(1)[0], 1)\n","        next_q = mask * next_q\n","        \n","        # TODO : Build Bellman target for Q-update\n","        target = rewards + gamma * next_q \n","\n","    out = agent.critic(observations).gather(1, actions)\n","\n","    loss_ftn = MSELoss()\n","    loss = loss_ftn(out, target)\n","\n","    critic_optim.zero_grad()\n","    loss.backward()\n","    critic_optim.step()\n","        \n","    # soft target update (both actor & critic network)\n","    for p, targ_p in zip(agent.critic.parameters(), target_critic.parameters()):\n","        targ_p.data.copy_((1. - tau) * targ_p + tau * p)\n","        \n","    return"]},{"cell_type":"markdown","metadata":{"id":"Dx4Um-dtRrdJ"},"source":["# 3. Combining these, we finally have..."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":301,"status":"ok","timestamp":1655363993808,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"ZWDysql-RrdI"},"outputs":[],"source":["def evaluate(agent, env, num_episodes=5):\n","\n","    sum_scores = 0.\n","    \n","    for i in range(num_episodes):\n","        obs = env.reset()\n","        done = False\n","        score = 0.\n","        \n","        for i in range(1000):\n","            action = agent.act(obs)\n","            obs, rew, done, _ = env.step(action)\n","            score += rew\n","            if done:\n","              break\n","        sum_scores += score\n","    avg_score = sum_scores / num_episodes\n","    \n","    return avg_score"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":286,"status":"ok","timestamp":1655363995436,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"V3I8TRN1RrdJ"},"outputs":[],"source":["def train(agent, env, gamma, \n","          lr, tau,\n","          ep_len, num_updates, batch_size,\n","          init_buffer=5000, buffer_size=100000,\n","          start_train=2000, train_interval=50,\n","          eval_interval=2000, snapshot_interval=10000,\n","          path=None):\n","    \n","    # Epsilon-greedy schedule\n","    max_epsilon = 1.\n","    min_epsilon = 0.02\n","    exploration_schedule = LinearSchedule(begin_t=start_train,\n","                                          end_t=num_updates,\n","                                          begin_value=max_epsilon,\n","                                          end_value=min_epsilon\n","                                         )\n","    \n","    # Environment for evaluation\n","    test_env = copy.deepcopy(env)\n","    \n","    # copy critic to build a target network (double DQN)\n","    target_critic = copy.deepcopy(agent.critic)\n","    for p in target_critic.parameters():\n","        p.requires_grad_(False)\n","\n","    # We load model by path variable\n","    if path is not None:\n","        recover_snapshot(path, agent.critic,\n","                         target_critic, critic_optim,\n","                         device=device\n","                        )\n","    \n","    # Import replay buffer\n","    obs_dim = env.observation_space.shape[0]\n","    num_act = env.action_space.n\n","    replay_buf = ReplayBuffer(obs_dim, buffer_size)\n","\n","    # Set optimizer to train Q-Network\n","    critic_optim = Adam(agent.critic.parameters(), lr=lr)\n","    \n","    # Logging\n","    save_path = './snapshots/'\n","    os.makedirs(save_path, exist_ok=True)\n","    os.makedirs('./learning_curves/', exist_ok=True)\n","    log_file = open('./learning_curves/res.csv',\n","                    'w',\n","                    encoding='utf-8',\n","                    newline=''\n","                   )\n","    logger = csv.writer(log_file)\n","    \n","    # Main loop\n","    obs = env.reset()\n","    done = False\n","    step_count = 0\n","    \n","    for t in range(num_updates + 1):\n","        if t < init_buffer:\n","            # This is not epsilon-greedy component!\n","            # perform random action until we collect sufficiently many samples\n","            # this is for exploration purpose\n","            action = env.action_space.sample()\n","        else:\n","            # executes epsilon-greedy action\n","            epsilon = exploration_schedule(t)\n","            action = agent.act(obs, epsilon=epsilon)\n","            \n","        next_obs, rew, done, _ = env.step(action)\n","        step_count += 1\n","        if step_count == ep_len:\n","            # if the next_state is not terminal but done is set to True by gym env wrapper\n","            done = False\n","            \n","        replay_buf.append(obs, action, next_obs, rew, done)\n","        obs = next_obs\n","        \n","        if done == True or step_count == ep_len:\n","            # reset environment if current environment reaches a terminal state \n","            # or step count reaches predefined length\n","            obs = env.reset()\n","            done = False\n","            step_count = 0\n","            # score = evaluate(agent, env)\n","            # print('[iteration {}] evaluation score : {}'.format(t, score))\n","        \n","        if t > start_train and t % train_interval == 0:\n","            # start training after fixed number of steps\n","            # this may mitigate overfitting of networks to the \n","            # small number of samples collected during the initial stage of training\n","            for _ in range(train_interval):\n","                update(agent,\n","                       replay_buf,\n","                       gamma,\n","                       critic_optim,\n","                       target_critic,\n","                       tau,\n","                       batch_size\n","                      )\n","                \n","        if t % eval_interval == 0:\n","            avg_score = evaluate(agent, test_env, num_episodes=5)\n","            print('[iter {}] average score = {} (over 5 episodes)'.format(t, avg_score))\n","            evaluation_log = [t, avg_score]\n","            logger.writerow(evaluation_log)\n","        \n","        if t % snapshot_interval == 0:\n","            snapshot_path = save_path + 'iter{}_'.format(t)\n","            # save weight & training progress\n","            save_snapshot(snapshot_path, agent.critic, target_critic, critic_optim)\n","\n","    log_file.close()"]},{"cell_type":"markdown","metadata":{"id":"WlIERr_hRrdK"},"source":["# 4. Let's train our agent!"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1655363999860,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"XvylvSlGRrdL","outputId":"5cdb92ac-f516-4265-97a5-26c9dde3f7a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["observation space dim. : 4 / # actions : 2\n"]}],"source":["env = gym.make('CartPole-v1')\n","obs_dim = env.observation_space.shape[0]\n","num_act = env.action_space.n\n","\n","print('observation space dim. : {} / # actions : {}'.format(obs_dim, num_act))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11325,"status":"ok","timestamp":1655364012755,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"xhEHS1IQwEua"},"outputs":[],"source":["# Hyperparameters\n","# [TODO] Optimize this to see effect\n","gamma = 0.99\n","lr = 1e-3\n","tau = 1e-3\n","ep_len = 500\n","num_updates = 100000\n","batch_size = 128\n","\n","agent = DQNAgent(obs_dim=obs_dim, num_act=num_act)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290645,"status":"ok","timestamp":1655364321232,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"5S_XZ9EvwXWy","outputId":"73bdcbe5-bec1-4967-d96a-23883b5bc3c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[iter 0] average score = 9.4 (over 5 episodes)\n","[iter 2000] average score = 9.4 (over 5 episodes)\n","[iter 4000] average score = 12.8 (over 5 episodes)\n","[iter 6000] average score = 161.8 (over 5 episodes)\n","[iter 8000] average score = 56.0 (over 5 episodes)\n","[iter 10000] average score = 134.6 (over 5 episodes)\n","[iter 12000] average score = 364.4 (over 5 episodes)\n","[iter 14000] average score = 145.6 (over 5 episodes)\n","[iter 16000] average score = 103.2 (over 5 episodes)\n","[iter 18000] average score = 105.2 (over 5 episodes)\n","[iter 20000] average score = 135.4 (over 5 episodes)\n","[iter 22000] average score = 110.0 (over 5 episodes)\n","[iter 24000] average score = 101.2 (over 5 episodes)\n","[iter 26000] average score = 169.6 (over 5 episodes)\n","[iter 28000] average score = 144.0 (over 5 episodes)\n","[iter 30000] average score = 137.0 (over 5 episodes)\n","[iter 32000] average score = 176.6 (over 5 episodes)\n","[iter 34000] average score = 168.2 (over 5 episodes)\n","[iter 36000] average score = 133.2 (over 5 episodes)\n","[iter 38000] average score = 209.4 (over 5 episodes)\n","[iter 40000] average score = 134.6 (over 5 episodes)\n","[iter 42000] average score = 214.4 (over 5 episodes)\n","[iter 44000] average score = 406.2 (over 5 episodes)\n","[iter 46000] average score = 198.6 (over 5 episodes)\n","[iter 48000] average score = 130.2 (over 5 episodes)\n","[iter 50000] average score = 122.8 (over 5 episodes)\n","[iter 52000] average score = 144.4 (over 5 episodes)\n","[iter 54000] average score = 128.0 (over 5 episodes)\n","[iter 56000] average score = 243.2 (over 5 episodes)\n","[iter 58000] average score = 103.8 (over 5 episodes)\n","[iter 60000] average score = 138.8 (over 5 episodes)\n","[iter 62000] average score = 216.8 (over 5 episodes)\n","[iter 64000] average score = 301.2 (over 5 episodes)\n","[iter 66000] average score = 100.4 (over 5 episodes)\n","[iter 68000] average score = 108.2 (over 5 episodes)\n","[iter 70000] average score = 197.6 (over 5 episodes)\n","[iter 72000] average score = 239.8 (over 5 episodes)\n","[iter 74000] average score = 120.4 (over 5 episodes)\n","[iter 76000] average score = 88.0 (over 5 episodes)\n","[iter 78000] average score = 189.0 (over 5 episodes)\n","[iter 80000] average score = 104.6 (over 5 episodes)\n","[iter 82000] average score = 238.0 (over 5 episodes)\n","[iter 84000] average score = 91.2 (over 5 episodes)\n","[iter 86000] average score = 138.2 (over 5 episodes)\n","[iter 88000] average score = 204.6 (over 5 episodes)\n","[iter 90000] average score = 102.2 (over 5 episodes)\n","[iter 92000] average score = 160.4 (over 5 episodes)\n","[iter 94000] average score = 435.2 (over 5 episodes)\n","[iter 96000] average score = 91.0 (over 5 episodes)\n","[iter 98000] average score = 315.0 (over 5 episodes)\n","[iter 100000] average score = 101.2 (over 5 episodes)\n"]}],"source":["train(agent, env, gamma, \n","      lr, tau,\n","      ep_len, num_updates, batch_size,\n","      init_buffer=5000, buffer_size=100000,\n","      start_train=2000, train_interval=50,\n","      eval_interval=2000, snapshot_interval=2000, path=None)"]},{"cell_type":"markdown","metadata":{"id":"PcnFa4TPRrdM"},"source":["# 5. Watch the trained agent!"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":764,"status":"ok","timestamp":1655364336842,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"62n0MP4lRrdC"},"outputs":[],"source":["colab=False\n","if colab:\n","    import gym\n","    from gym.wrappers import Monitor\n","    import glob\n","    import io\n","    import base64\n","    from IPython.display import HTML\n","    from pyvirtualdisplay import Display\n","    from IPython import display as ipythondisplay\n","\n","    display = Display(visible=0, size=(1400, 900))\n","    display.start()\n","\n","    def show_video():\n","      mp4list = glob.glob('video/*.mp4')\n","      if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","                </video>'''.format(encoded.decode('ascii'))))\n","      else: \n","        print(\"Could not find video\")\n","        \n","\n","    def wrap_env(env):\n","      env = Monitor(env, './video', force=True)\n","      return env\n","\n","    env = wrap_env(env)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"executionInfo":{"elapsed":5653,"status":"ok","timestamp":1655364367665,"user":{"displayName":"­박민규 / 학생 / 전기·정보공학부","userId":"15454198223134513846"},"user_tz":-540},"id":"CxSDgFszRrdM","outputId":"22dff2d8-57fa-43f4-b265-10f0d6d06449"},"outputs":[{"ename":"NameError","evalue":"name 'gym' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/kky8822/2022_AI_expert/0617/chap4_dqn/dqn.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kky8822/2022_AI_expert/0617/chap4_dqn/dqn.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kky8822/2022_AI_expert/0617/chap4_dqn/dqn.ipynb#ch0000002vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m colab:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kky8822/2022_AI_expert/0617/chap4_dqn/dqn.ipynb#ch0000002vscode-remote?line=2'>3</a>\u001b[0m   env \u001b[39m=\u001b[39m wrap_env(env)\n","\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"]}],"source":["env = gym.make('CartPole-v1')\n","if colab:\n","  env = wrap_env(env)\n","obs = env.reset()\n","done = False\n","score = 0.\n","load_model(agent, path='./snapshots/iter94000_model.pth.tar', device=device)\n","for i in range(1000):\n","    env.render()\n","    obs, rew, done, _ = env.step(agent.act(obs))\n","    score += rew\n","    if done:\n","      break\n","    \n","env.close()\n","print('score : ', score)\n","\n","if colab:\n","    show_video()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"dqn.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.4 ('2022_AI_expert-VdvU_stD')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"6f9b4e903e38bfd606aa85746352f0c68a834a97143e8a5ff660a0ae957bcf1b"}}},"nbformat":4,"nbformat_minor":0}
