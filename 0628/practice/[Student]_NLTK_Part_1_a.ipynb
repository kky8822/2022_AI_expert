{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ccwHP6sdUy"
      },
      "source": [
        "# **References**\n",
        "This tutorial is based on https://github.com/hb20007/hands-on-nltk-tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhklFH83P_RZ"
      },
      "source": [
        "# 1.1 **Downloading NLTK Libraries:** *Getting ready to start!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T5IlY3igMlg5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package reuters to /home/kky8822/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package brown to /home/kky8822/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package tagsets to /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package names to /home/kky8822/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
            "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
            "unzip:  cannot find or open /root/nltk_data/corpora/reuters.zip, /root/nltk_data/corpora/reuters.zip.zip or /root/nltk_data/corpora/reuters.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# We install and import necessary python libraries\n",
        "\n",
        "# !pipenv install nltk TwitterSearch unidecode langdetect langid gensim tweepy\n",
        "\n",
        "import nltk # https://www.nltk.org/install.html\n",
        "import numpy # https://www.scipy.org/install.html\n",
        "import matplotlib.pyplot # https://matplotlib.org/downloads.html\n",
        "import tweepy # https://github.com/tweepy/tweepy\n",
        "import TwitterSearch # https://github.com/ckoepp/TwitterSearch\n",
        "import unidecode # https://pypi.python.org/pypi/Unidecode\n",
        "import langdetect # https://pypi.python.org/pypi/langdetect\n",
        "import langid # https://github.com/saffsd/langid.py\n",
        "import gensim # https://radimrehurek.com/gensim/install.html\n",
        "\n",
        "\n",
        "# Nltk has many extra functionalities that can be downloaded\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('reuters')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('brown')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('names')\n",
        "\n",
        "!apt-get install unzip\n",
        "!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "#!unzip /root/nltk_data/corpora/stopwords.zip -d /root/nltk_data/corpora\n",
        "#from nltk.corpus import reuters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k9hvbAeNsN-"
      },
      "source": [
        "# 1.2 **Text Analysis Using nltk:** *Extracting interesting data from a given text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnxRGmSoNsN-"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.text import Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzRvfGYQNsN_"
      },
      "outputs": [],
      "source": [
        "my_string = \"Two plus two is four, minus one that's three — quick maths. Every day man's on the block. Smoke trees. See your girl in the park, that girl is an uckers. When the thing went quack quack quack, your men were ducking! Hold tight Asznee, my brother. He's got a pumpy. Hold tight my man, my guy. He's got a frisbee. I trap, trap, trap on the phone. Moving that cornflakes, rice crispies. Hold tight my girl Whitney.\"\n",
        "tokens = word_tokenize(my_string)\n",
        "tokens = [word.lower() for word in tokens]\n",
        "tokens[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K0ULvApNsOA"
      },
      "outputs": [],
      "source": [
        "t = Text(tokens)\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mik_B6PjNsOA"
      },
      "source": [
        "This method of converting raw strings to NLTK `Text` instances can be used when reading text from a file. For instance:\n",
        "```python\n",
        "f = open('my-file.txt','rU') # Opening a file with the mode 'U' or 'rU' will open a file for reading in universal newline mode. All three line ending conventions will be translated to a \"\\n\"\n",
        "raw = f.read()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAWv6aTRNsOA"
      },
      "outputs": [],
      "source": [
        "# concordance() is a method of the Text class of NLTK. It finds words and displays a context window. Word matching is not case-sensitive.\n",
        "t.concordance('uckers') \n",
        "# concordance() is defined as follows: concordance(self, word, width=79, lines=25). Note default values for optional params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thnsKp6fNsOB"
      },
      "outputs": [],
      "source": [
        "# Collocations are expressions of multiple words which commonly co-occur.\n",
        "t.collocations() # def collocations(self, num=20, window_size=2). num is the max no. of collocations to print."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAcW1ZzsNsOC"
      },
      "outputs": [],
      "source": [
        "t.count('quack')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cZczZ0lNsOC"
      },
      "outputs": [],
      "source": [
        "t.index('two')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSF-WZ8-NsOD"
      },
      "outputs": [],
      "source": [
        "t.similar('brother') # similar(self, word, num=20). Distributional similarity: find other words which appear in the same contexts as the specified word; list most similar words first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKGZKpm1NsOE"
      },
      "outputs": [],
      "source": [
        "t.dispersion_plot(['man', 'thing', 'quack']) # Reveals patterns in word positions. Each stripe represents an instance of a word, and each row represents the entire text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I08iXdkKNsOF"
      },
      "outputs": [],
      "source": [
        "t.plot(20) # plots 20 most common tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3GVkfw2NsOG"
      },
      "outputs": [],
      "source": [
        "t.vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ZsrHhjNsOG"
      },
      "source": [
        "Another thing that might be useful in analysis is finding common contexts. Our text is too small so we will use a bigger one.\n",
        "\n",
        "NLTK comes with several interesting **corpora**, which are large collections of text. You can check out what kinds of corpora are found in `nltk.corpus` in Section 1 [here](http://www.nltk.org/book/ch02.html).\n",
        "\n",
        "`reuters` is a corpus of news documents. More specifically, `reuters` is a *corpus reader* for the Reuters corpus which provides us with methods to access the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqLCXvGpNsOH"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import reuters\n",
        "text = Text(reuters.words()) # .words() is one method corpus readers provide for reading data from a corpus. We will learn more about these methods in Chapter 2.\n",
        "text.common_contexts(['August', 'June']) # It seems that .common_contexts() takes 2 words which are used similarly and displays where they are used similarly. It also seems that '_' indicates where the words would be in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlgBKL88NsOJ"
      },
      "source": [
        "We will further explore the Reuters corpus as well as several others in later chapters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usCQ3m5MVhYn"
      },
      "source": [
        "# 1.3 Hands-on: Try NLTK functions on Shakespeare's poems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxKUVPqTVeiz"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt\n",
        "shakespeare = open(\"shakespeare.txt\").readlines() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu8BZszJV15L"
      },
      "outputs": [],
      "source": [
        "len(shakespeare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZN7ZngMWCbj"
      },
      "outputs": [],
      "source": [
        "for line in shakespeare[:50]:\n",
        "  print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9-9rdLhWM6S"
      },
      "outputs": [],
      "source": [
        "small_shakespeare = shakespeare[500:2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyhevjO7WhVR"
      },
      "outputs": [],
      "source": [
        "# This is a list of sentences\n",
        "small_shakespeare[:5] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78Fu2sAGWjkK"
      },
      "outputs": [],
      "source": [
        "# We want to get the tokens\n",
        "small_shakespeare_tokens = []\n",
        "\n",
        "# Give it a try:\n",
        "small_shakespeare_tokens = word_tokenize(' '.join(small_shakespeare))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1VM7URvWW6z"
      },
      "outputs": [],
      "source": [
        "t = Text(small_shakespeare_tokens)\n",
        "small_shakespeare_tokens[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2zzR_uSWY9U"
      },
      "outputs": [],
      "source": [
        "t.collocations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRWB3AcZefq"
      },
      "source": [
        "# 1.4 More NLTK: Advanced Functions\n",
        "\n",
        "These are from the [Official NLTK Documentation](https://www.nltk.org/howto)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjb85RCZce-x"
      },
      "source": [
        "## Collocations advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMshMjQ_Z4r1"
      },
      "outputs": [],
      "source": [
        "#import nltk\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()\n",
        "nltk.download('genesis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXepgSGxaDew"
      },
      "outputs": [],
      "source": [
        "finder = BigramCollocationFinder.from_words(nltk.corpus.genesis.words('english-web.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFTsB_a5bIyK"
      },
      "outputs": [],
      "source": [
        "# https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf\n",
        "# Mutual information (MI) is a measure of the information overlap between two random variables\n",
        "# Pointwise mutual information (PMI, 5) is a measure of \n",
        "# how much the actual probability of a particular co-occurrence of events p(x, y) \n",
        "# differs from what wewould expect it to be on the basis of the probabilities of the individual events and the assumption of independence p(x)p(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHcZ-S0Caib6"
      },
      "outputs": [],
      "source": [
        "# top ten bigram collocations in Genesis\n",
        "finder.nbest(bigram_measures.pmi, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNgsrzd5bjFD"
      },
      "outputs": [],
      "source": [
        "# While these words are highly collocated, the expressions are also very infrequent. \n",
        "# Therefore it is useful to apply filters, such as ignoring all bigrams which occur less than three times in the corpus:\n",
        "finder.apply_freq_filter(3)\n",
        "finder.nbest(bigram_measures.pmi, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TGYTe-malo2"
      },
      "outputs": [],
      "source": [
        "# We may similarly find collocations among tagged words:\n",
        "nltk.download('universal_tagset')\n",
        "finder = BigramCollocationFinder.from_words(nltk.corpus.brown.tagged_words('ca01', tagset='universal'))\n",
        "finder.nbest(bigram_measures.pmi, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q1n8fiAcCPF"
      },
      "outputs": [],
      "source": [
        "# Or tags alone:\n",
        "finder = BigramCollocationFinder.from_words(t for w, t in nltk.corpus.brown.tagged_words('ca01', tagset='universal'))\n",
        "finder.nbest(bigram_measures.pmi, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrNAzzLGcIIP"
      },
      "outputs": [],
      "source": [
        "# Or spanning intervening words:\n",
        "finder = BigramCollocationFinder.from_words(\n",
        "    nltk.corpus.genesis.words('english-web.txt'),\n",
        "     window_size = 20)\n",
        "finder.apply_freq_filter(2)\n",
        "ignored_words = nltk.corpus.stopwords.words('english')\n",
        "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
        "finder.nbest(bigram_measures.likelihood_ratio, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBhDETBkcbBm"
      },
      "source": [
        "## TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Z20H92m6Kh"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/justmarkham/pycon-2019-tutorial/master/ted.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuD4NZKVm9LA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./ted.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6odvGnHnCy5"
      },
      "outputs": [],
      "source": [
        "ted = df['description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP6KZUhpnMVb"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(ted)\n",
        "\n",
        "# Print the shape of tfidf_matrix\n",
        "print(tfidf_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sscKu3PVcjNT"
      },
      "outputs": [],
      "source": [
        "print(\"Token's used as Features \")\n",
        "print(vectorizer.get_feature_names(),\"\\n\")\n",
        "print(\"Size of the array\")\n",
        "print(tfidf_matrix.shape,\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d__i2ePIhNCS"
      },
      "outputs": [],
      "source": [
        "ted.iloc[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLjIoSgniblG"
      },
      "outputs": [],
      "source": [
        "vec1 = vectorizer.transform([ted.iloc[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbyjAHHmihnG"
      },
      "outputs": [],
      "source": [
        "all_vecs = vectorizer.transform(ted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKZRUH7_lZGM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "all_scores = [cosine_similarity(vec1, vec) for vec in all_vecs ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su8UAWt5occg"
      },
      "outputs": [],
      "source": [
        "all_scores[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDReQI2CohbF"
      },
      "outputs": [],
      "source": [
        "all_scores_list = [item[0][0] for item in all_scores]\n",
        "print(all_scores_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX1nA0HgoKRW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "ranked = np.argsort(all_scores_list)\n",
        "print(ranked[-5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3r3KMaOoWk0"
      },
      "outputs": [],
      "source": [
        "for idx in ranked[-5:]:\n",
        "  print(ted.iloc[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUu8LtImporA"
      },
      "outputs": [],
      "source": [
        "# Try different queries or datasets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8fp5cUySn-u"
      },
      "source": [
        "# 1.5 Brief Detour: Recent research on Zipf's distribution\n",
        "\n",
        "[A recent research paper on Zipf's distribution](https://docs.google.com/presentation/d/1HL-TfYil5u6lzoCV0YMJOEggrKuZJdqa68EgvrAKcPE/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-LY80jiMPPU"
      },
      "source": [
        "# 1.5 Classifying News Documents into Categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZKk7FqMMPPW"
      },
      "source": [
        "Based on *Another Excercise: Classifying News Documents in Categories: sport, humor, adventure, science fiction, etc...* in [Natural Language Processing with Python/NLTK by Luciano M. Guasco](https://github.com/luchux/ipython-notebook-nltk/blob/master/NLP%20-%20MelbDjango.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCrN07QmMPPX"
      },
      "source": [
        "## 1. Exploring the `brown` corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8zxggHNMPPX"
      },
      "source": [
        "The Corpus consists of 500 samples, distributed across 15 genres. Each sample began at a random sentence-boundary in the article or other unit chosen, and continued up to the first sentence boundary after 2,000 words.\n",
        "\n",
        "- **A.** PRESS: Reportage *(44 texts)*\n",
        "- **B.** PRESS: Editorial *(27 texts)*\n",
        "- **C.** PRESS: Reviews *(17 texts)*\n",
        "- **D.** RELIGION *(17 texts)*\n",
        "- **E.** SKILL AND HOBBIES *(36 texts)*\n",
        "- **F.** POPULAR LORE *(48 texts)*\n",
        "- **G.** BELLES-LETTRES - Biography, Memoirs, etc. *(75 texts)*\n",
        "- **H.** MISCELLANEOUS: US Government & House Organs *(30 texts)*\n",
        "- **J.** LEARNED - Natural sciences, Medicine, Mathematics, etc. *(80 texts)*\n",
        "- **K.** FICTION: General *(29 texts)*\n",
        "- **L.** FICTION: Mystery and Detective Fiction *(24 texts)*\n",
        "- **M.** FICTION: Science *(6 texts)*\n",
        "- **N.** FICTION: Adventure and Western *(29 texts)*\n",
        "- **P.** FICTION: Romance and Love Story *(29 texts)*\n",
        "- **R.** HUMOR *(9 texts)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bckNBCQjMPPY",
        "outputId": "7c7a8158-1359-4d48-d519-4401d3200bc5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "DKQyqU_TMPPY",
        "outputId": "ba7ba49b-dba2-47f2-c674-866552379133"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'BROWN CORPUS  A Standard Corpus of Present-Day Edited American English, for use with Digital Computers.  by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA  Revised 1971, Revised and Amplified 1979  http://www.hit.uib.no/icame/brown/bcm.html  Distributed with the permission of the copyright holder, redistribution permitted. '"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brown.readme().replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Q5ECB2MPPZ",
        "outputId": "2f1575e3-b667-411a-bcb1-4c71cfe28f55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ca01',\n",
              " 'ca02',\n",
              " 'ca03',\n",
              " 'ca04',\n",
              " 'ca05',\n",
              " 'ca06',\n",
              " 'ca07',\n",
              " 'ca08',\n",
              " 'ca09',\n",
              " 'ca10',\n",
              " 'ca11',\n",
              " 'ca12',\n",
              " 'ca13',\n",
              " 'ca14',\n",
              " 'ca15',\n",
              " 'ca16',\n",
              " 'ca17',\n",
              " 'ca18',\n",
              " 'ca19',\n",
              " 'ca20',\n",
              " 'ca21',\n",
              " 'ca22',\n",
              " 'ca23',\n",
              " 'ca24',\n",
              " 'ca25',\n",
              " 'ca26',\n",
              " 'ca27',\n",
              " 'ca28',\n",
              " 'ca29',\n",
              " 'ca30',\n",
              " 'ca31',\n",
              " 'ca32',\n",
              " 'ca33',\n",
              " 'ca34',\n",
              " 'ca35',\n",
              " 'ca36',\n",
              " 'ca37',\n",
              " 'ca38',\n",
              " 'ca39',\n",
              " 'ca40',\n",
              " 'ca41',\n",
              " 'ca42',\n",
              " 'ca43',\n",
              " 'ca44',\n",
              " 'cb01',\n",
              " 'cb02',\n",
              " 'cb03',\n",
              " 'cb04',\n",
              " 'cb05',\n",
              " 'cb06',\n",
              " 'cb07',\n",
              " 'cb08',\n",
              " 'cb09',\n",
              " 'cb10',\n",
              " 'cb11',\n",
              " 'cb12',\n",
              " 'cb13',\n",
              " 'cb14',\n",
              " 'cb15',\n",
              " 'cb16',\n",
              " 'cb17',\n",
              " 'cb18',\n",
              " 'cb19',\n",
              " 'cb20',\n",
              " 'cb21',\n",
              " 'cb22',\n",
              " 'cb23',\n",
              " 'cb24',\n",
              " 'cb25',\n",
              " 'cb26',\n",
              " 'cb27',\n",
              " 'cc01',\n",
              " 'cc02',\n",
              " 'cc03',\n",
              " 'cc04',\n",
              " 'cc05',\n",
              " 'cc06',\n",
              " 'cc07',\n",
              " 'cc08',\n",
              " 'cc09',\n",
              " 'cc10',\n",
              " 'cc11',\n",
              " 'cc12',\n",
              " 'cc13',\n",
              " 'cc14',\n",
              " 'cc15',\n",
              " 'cc16',\n",
              " 'cc17',\n",
              " 'cd01',\n",
              " 'cd02',\n",
              " 'cd03',\n",
              " 'cd04',\n",
              " 'cd05',\n",
              " 'cd06',\n",
              " 'cd07',\n",
              " 'cd08',\n",
              " 'cd09',\n",
              " 'cd10',\n",
              " 'cd11',\n",
              " 'cd12',\n",
              " 'cd13',\n",
              " 'cd14',\n",
              " 'cd15',\n",
              " 'cd16',\n",
              " 'cd17',\n",
              " 'ce01',\n",
              " 'ce02',\n",
              " 'ce03',\n",
              " 'ce04',\n",
              " 'ce05',\n",
              " 'ce06',\n",
              " 'ce07',\n",
              " 'ce08',\n",
              " 'ce09',\n",
              " 'ce10',\n",
              " 'ce11',\n",
              " 'ce12',\n",
              " 'ce13',\n",
              " 'ce14',\n",
              " 'ce15',\n",
              " 'ce16',\n",
              " 'ce17',\n",
              " 'ce18',\n",
              " 'ce19',\n",
              " 'ce20',\n",
              " 'ce21',\n",
              " 'ce22',\n",
              " 'ce23',\n",
              " 'ce24',\n",
              " 'ce25',\n",
              " 'ce26',\n",
              " 'ce27',\n",
              " 'ce28',\n",
              " 'ce29',\n",
              " 'ce30',\n",
              " 'ce31',\n",
              " 'ce32',\n",
              " 'ce33',\n",
              " 'ce34',\n",
              " 'ce35',\n",
              " 'ce36',\n",
              " 'cf01',\n",
              " 'cf02',\n",
              " 'cf03',\n",
              " 'cf04',\n",
              " 'cf05',\n",
              " 'cf06',\n",
              " 'cf07',\n",
              " 'cf08',\n",
              " 'cf09',\n",
              " 'cf10',\n",
              " 'cf11',\n",
              " 'cf12',\n",
              " 'cf13',\n",
              " 'cf14',\n",
              " 'cf15',\n",
              " 'cf16',\n",
              " 'cf17',\n",
              " 'cf18',\n",
              " 'cf19',\n",
              " 'cf20',\n",
              " 'cf21',\n",
              " 'cf22',\n",
              " 'cf23',\n",
              " 'cf24',\n",
              " 'cf25',\n",
              " 'cf26',\n",
              " 'cf27',\n",
              " 'cf28',\n",
              " 'cf29',\n",
              " 'cf30',\n",
              " 'cf31',\n",
              " 'cf32',\n",
              " 'cf33',\n",
              " 'cf34',\n",
              " 'cf35',\n",
              " 'cf36',\n",
              " 'cf37',\n",
              " 'cf38',\n",
              " 'cf39',\n",
              " 'cf40',\n",
              " 'cf41',\n",
              " 'cf42',\n",
              " 'cf43',\n",
              " 'cf44',\n",
              " 'cf45',\n",
              " 'cf46',\n",
              " 'cf47',\n",
              " 'cf48',\n",
              " 'cg01',\n",
              " 'cg02',\n",
              " 'cg03',\n",
              " 'cg04',\n",
              " 'cg05',\n",
              " 'cg06',\n",
              " 'cg07',\n",
              " 'cg08',\n",
              " 'cg09',\n",
              " 'cg10',\n",
              " 'cg11',\n",
              " 'cg12',\n",
              " 'cg13',\n",
              " 'cg14',\n",
              " 'cg15',\n",
              " 'cg16',\n",
              " 'cg17',\n",
              " 'cg18',\n",
              " 'cg19',\n",
              " 'cg20',\n",
              " 'cg21',\n",
              " 'cg22',\n",
              " 'cg23',\n",
              " 'cg24',\n",
              " 'cg25',\n",
              " 'cg26',\n",
              " 'cg27',\n",
              " 'cg28',\n",
              " 'cg29',\n",
              " 'cg30',\n",
              " 'cg31',\n",
              " 'cg32',\n",
              " 'cg33',\n",
              " 'cg34',\n",
              " 'cg35',\n",
              " 'cg36',\n",
              " 'cg37',\n",
              " 'cg38',\n",
              " 'cg39',\n",
              " 'cg40',\n",
              " 'cg41',\n",
              " 'cg42',\n",
              " 'cg43',\n",
              " 'cg44',\n",
              " 'cg45',\n",
              " 'cg46',\n",
              " 'cg47',\n",
              " 'cg48',\n",
              " 'cg49',\n",
              " 'cg50',\n",
              " 'cg51',\n",
              " 'cg52',\n",
              " 'cg53',\n",
              " 'cg54',\n",
              " 'cg55',\n",
              " 'cg56',\n",
              " 'cg57',\n",
              " 'cg58',\n",
              " 'cg59',\n",
              " 'cg60',\n",
              " 'cg61',\n",
              " 'cg62',\n",
              " 'cg63',\n",
              " 'cg64',\n",
              " 'cg65',\n",
              " 'cg66',\n",
              " 'cg67',\n",
              " 'cg68',\n",
              " 'cg69',\n",
              " 'cg70',\n",
              " 'cg71',\n",
              " 'cg72',\n",
              " 'cg73',\n",
              " 'cg74',\n",
              " 'cg75',\n",
              " 'ch01',\n",
              " 'ch02',\n",
              " 'ch03',\n",
              " 'ch04',\n",
              " 'ch05',\n",
              " 'ch06',\n",
              " 'ch07',\n",
              " 'ch08',\n",
              " 'ch09',\n",
              " 'ch10',\n",
              " 'ch11',\n",
              " 'ch12',\n",
              " 'ch13',\n",
              " 'ch14',\n",
              " 'ch15',\n",
              " 'ch16',\n",
              " 'ch17',\n",
              " 'ch18',\n",
              " 'ch19',\n",
              " 'ch20',\n",
              " 'ch21',\n",
              " 'ch22',\n",
              " 'ch23',\n",
              " 'ch24',\n",
              " 'ch25',\n",
              " 'ch26',\n",
              " 'ch27',\n",
              " 'ch28',\n",
              " 'ch29',\n",
              " 'ch30',\n",
              " 'cj01',\n",
              " 'cj02',\n",
              " 'cj03',\n",
              " 'cj04',\n",
              " 'cj05',\n",
              " 'cj06',\n",
              " 'cj07',\n",
              " 'cj08',\n",
              " 'cj09',\n",
              " 'cj10',\n",
              " 'cj11',\n",
              " 'cj12',\n",
              " 'cj13',\n",
              " 'cj14',\n",
              " 'cj15',\n",
              " 'cj16',\n",
              " 'cj17',\n",
              " 'cj18',\n",
              " 'cj19',\n",
              " 'cj20',\n",
              " 'cj21',\n",
              " 'cj22',\n",
              " 'cj23',\n",
              " 'cj24',\n",
              " 'cj25',\n",
              " 'cj26',\n",
              " 'cj27',\n",
              " 'cj28',\n",
              " 'cj29',\n",
              " 'cj30',\n",
              " 'cj31',\n",
              " 'cj32',\n",
              " 'cj33',\n",
              " 'cj34',\n",
              " 'cj35',\n",
              " 'cj36',\n",
              " 'cj37',\n",
              " 'cj38',\n",
              " 'cj39',\n",
              " 'cj40',\n",
              " 'cj41',\n",
              " 'cj42',\n",
              " 'cj43',\n",
              " 'cj44',\n",
              " 'cj45',\n",
              " 'cj46',\n",
              " 'cj47',\n",
              " 'cj48',\n",
              " 'cj49',\n",
              " 'cj50',\n",
              " 'cj51',\n",
              " 'cj52',\n",
              " 'cj53',\n",
              " 'cj54',\n",
              " 'cj55',\n",
              " 'cj56',\n",
              " 'cj57',\n",
              " 'cj58',\n",
              " 'cj59',\n",
              " 'cj60',\n",
              " 'cj61',\n",
              " 'cj62',\n",
              " 'cj63',\n",
              " 'cj64',\n",
              " 'cj65',\n",
              " 'cj66',\n",
              " 'cj67',\n",
              " 'cj68',\n",
              " 'cj69',\n",
              " 'cj70',\n",
              " 'cj71',\n",
              " 'cj72',\n",
              " 'cj73',\n",
              " 'cj74',\n",
              " 'cj75',\n",
              " 'cj76',\n",
              " 'cj77',\n",
              " 'cj78',\n",
              " 'cj79',\n",
              " 'cj80',\n",
              " 'ck01',\n",
              " 'ck02',\n",
              " 'ck03',\n",
              " 'ck04',\n",
              " 'ck05',\n",
              " 'ck06',\n",
              " 'ck07',\n",
              " 'ck08',\n",
              " 'ck09',\n",
              " 'ck10',\n",
              " 'ck11',\n",
              " 'ck12',\n",
              " 'ck13',\n",
              " 'ck14',\n",
              " 'ck15',\n",
              " 'ck16',\n",
              " 'ck17',\n",
              " 'ck18',\n",
              " 'ck19',\n",
              " 'ck20',\n",
              " 'ck21',\n",
              " 'ck22',\n",
              " 'ck23',\n",
              " 'ck24',\n",
              " 'ck25',\n",
              " 'ck26',\n",
              " 'ck27',\n",
              " 'ck28',\n",
              " 'ck29',\n",
              " 'cl01',\n",
              " 'cl02',\n",
              " 'cl03',\n",
              " 'cl04',\n",
              " 'cl05',\n",
              " 'cl06',\n",
              " 'cl07',\n",
              " 'cl08',\n",
              " 'cl09',\n",
              " 'cl10',\n",
              " 'cl11',\n",
              " 'cl12',\n",
              " 'cl13',\n",
              " 'cl14',\n",
              " 'cl15',\n",
              " 'cl16',\n",
              " 'cl17',\n",
              " 'cl18',\n",
              " 'cl19',\n",
              " 'cl20',\n",
              " 'cl21',\n",
              " 'cl22',\n",
              " 'cl23',\n",
              " 'cl24',\n",
              " 'cm01',\n",
              " 'cm02',\n",
              " 'cm03',\n",
              " 'cm04',\n",
              " 'cm05',\n",
              " 'cm06',\n",
              " 'cn01',\n",
              " 'cn02',\n",
              " 'cn03',\n",
              " 'cn04',\n",
              " 'cn05',\n",
              " 'cn06',\n",
              " 'cn07',\n",
              " 'cn08',\n",
              " 'cn09',\n",
              " 'cn10',\n",
              " 'cn11',\n",
              " 'cn12',\n",
              " 'cn13',\n",
              " 'cn14',\n",
              " 'cn15',\n",
              " 'cn16',\n",
              " 'cn17',\n",
              " 'cn18',\n",
              " 'cn19',\n",
              " 'cn20',\n",
              " 'cn21',\n",
              " 'cn22',\n",
              " 'cn23',\n",
              " 'cn24',\n",
              " 'cn25',\n",
              " 'cn26',\n",
              " 'cn27',\n",
              " 'cn28',\n",
              " 'cn29',\n",
              " 'cp01',\n",
              " 'cp02',\n",
              " 'cp03',\n",
              " 'cp04',\n",
              " 'cp05',\n",
              " 'cp06',\n",
              " 'cp07',\n",
              " 'cp08',\n",
              " 'cp09',\n",
              " 'cp10',\n",
              " 'cp11',\n",
              " 'cp12',\n",
              " 'cp13',\n",
              " 'cp14',\n",
              " 'cp15',\n",
              " 'cp16',\n",
              " 'cp17',\n",
              " 'cp18',\n",
              " 'cp19',\n",
              " 'cp20',\n",
              " 'cp21',\n",
              " 'cp22',\n",
              " 'cp23',\n",
              " 'cp24',\n",
              " 'cp25',\n",
              " 'cp26',\n",
              " 'cp27',\n",
              " 'cp28',\n",
              " 'cp29',\n",
              " 'cr01',\n",
              " 'cr02',\n",
              " 'cr03',\n",
              " 'cr04',\n",
              " 'cr05',\n",
              " 'cr06',\n",
              " 'cr07',\n",
              " 'cr08',\n",
              " 'cr09']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brown.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skgRdXDGMPPa"
      },
      "outputs": [],
      "source": [
        "brown.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzR4cj5PMPPa"
      },
      "outputs": [],
      "source": [
        "brown.sents('ca01')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xI30fBfVwLp"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "           (genre, word)\n",
        "           for genre in brown.categories()\n",
        "           for word in brown.words(categories=genre))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNxQAb7MV1bk"
      },
      "outputs": [],
      "source": [
        "genre_word = [(genre, word) \n",
        "              for genre in ['news', 'romance']\n",
        "              for word in brown.words(categories=genre)] \n",
        "len(genre_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_LhGr5MV-UK"
      },
      "outputs": [],
      "source": [
        " genre_word[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwNkTcyjWARV"
      },
      "outputs": [],
      "source": [
        "genre_word[-10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n8VJ700WGDo"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(genre_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE2lPZj0WHP-"
      },
      "outputs": [],
      "source": [
        "cfd.conditions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q0gpBXzWPwl"
      },
      "outputs": [],
      "source": [
        "# Let's access the two conditions, and confirm that each is just a frequency distribution:\n",
        "print(cfd['news'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFQJIyO1WVCL"
      },
      "outputs": [],
      "source": [
        "print(cfd['romance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaTJCYAtWXHg"
      },
      "outputs": [],
      "source": [
        "cfd['romance'].most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOyc86E8WbZM"
      },
      "outputs": [],
      "source": [
        "cfd['romance']['could']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrndfNU6MPPb"
      },
      "source": [
        "## 2. Compiling list of most popular words in corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dhD2CrqMPPb"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist # Takes a bunch of tokens and returns the frequencies of all unique cases.\n",
        "words_in_corpora = FreqDist(w.lower() for w in brown.words() if w.isalpha()) # Checking if the word is alphabetical avoids including stuff like `` and '' which are actually pretty common. Note that it also omits words such as 1 (very common), aug., 1913, $30, 13th, over-all etc. Another option would have been .isalnum().\n",
        "words_in_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-b_mQBnMPPb"
      },
      "outputs": [],
      "source": [
        "words_in_corpora_freq_sorted = list(map(list, words_in_corpora.items())) # I use this instead of sorted() because I want to sort my dictionary into a (mutable) list in order to delete the second column as opposed to into a tuple (immutable).\n",
        "words_in_corpora_freq_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP8SKLb9MPPc"
      },
      "outputs": [],
      "source": [
        "words_in_corpora_freq_sorted.sort(key=lambda x: x[1], reverse=True) # Using a lambda function is an alternative to using the operator library.\n",
        "words_in_corpora_freq_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuZM0mtQMPPc"
      },
      "outputs": [],
      "source": [
        "best1500 = words_in_corpora_freq_sorted[:1500]\n",
        "\n",
        "for list_item in best1500:\n",
        "    del list_item[1]\n",
        "\n",
        "best1500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRek_ZWWMPPd"
      },
      "outputs": [],
      "source": [
        "# Since best1500 is now a list of words, it should be flattened.\n",
        "import itertools\n",
        "chain = itertools.chain(*best1500) # We break down the list into its individual sublists and then chain them. What chain does is that it further breaks down each sublist into its individual components so this approach can be used to flatten any list of lists.\n",
        "best1500 = list(chain) # chain is of type itertools.chain so we need the cast\n",
        "best1500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLJ8bRXXMPPd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopw = stopwords.words('english')\n",
        "\n",
        "# Receives a list of words and removes stop words from list\n",
        "def nonstop(listwords):\n",
        "    return [word for word in listwords if word not in stopw]\n",
        "\n",
        "best1500_words_corpora = nonstop(best1500) # Note how this will probably contain less than 1500 words.\n",
        "best1500_words_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUonJLFPMPPe"
      },
      "source": [
        "## 3. Converting corpus to form suitable for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRALpWsnMPPe"
      },
      "source": [
        "Each file in the corpus will eventually be represented by a dictionary showing the presence of the corpus’ most popular words in the particular file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv8mkUClMPPe"
      },
      "outputs": [],
      "source": [
        "# documents = [(nonstop(brown.words(fileid)), category) for category in brown.categories() for fileid in brown.fileids(category)]\n",
        "# documents # Note how documents is a list of tuples.\n",
        "\n",
        "# The code above generates a representation of the corpus but without removing punctuation. This is better:\n",
        "documents = [([item.lower() for item in nonstop(brown.words(fileid)) if item.isalpha()], category)\n",
        "             for category in brown.categories()\n",
        "             for fileid in brown.fileids(category)]\n",
        "documents # Note how documents is a list of tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DYk_fv3MPQG"
      },
      "outputs": [],
      "source": [
        "from random import shuffle\n",
        "\n",
        "shuffle(documents)\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5j6TDDEMPQd"
      },
      "outputs": [],
      "source": [
        "# Given a document extract features (the presence or not of the 1500 most frequent words of the corpus)\n",
        "def document_features(doc):\n",
        "    doc_set_words = set(doc) # Checking whether a word occurs in a set is much faster than checking whether it occurs in a list.\n",
        "    features_dic = {} # Features is a dictionary\n",
        "    for word in best1500_words_corpora:\n",
        "        features_dic['has(%s)' % word] = (word in doc_set_words)\n",
        "    return features_dic\n",
        "\n",
        "doc_features_set = [(document_features(d),c) for (d,c) in documents]\n",
        "doc_features_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f27Uz1uDMPQ5"
      },
      "source": [
        "## 4. Building classifier (Naive Bayes)\n",
        "\n",
        "\n",
        "*   Bayes 사용해서 P(x|c)와 P(c) 만으로 P(c|x)를 계산\n",
        "\n",
        "*   e.g. P( viagra,free | **Spam** ) 로부터 -> P( **Spam** | viagra,free )\n",
        "\n",
        "*   *Likelihood*: Spam이메일일 경우, viagra,free 단어가 있을 확률\n",
        "\n",
        "*   Maximum aposteriori estimation (MAP): 간단히 Posterior probability를 maximize하는 class를 선택 (inference시에)\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://blog.tenthplanet.in/wp-content/uploads/2019/01/6.png\" alt=\"seq2seq\" style=\"width: 60%\"/>\n",
        "\n",
        "*   **\"Naive\"** assumption -> P( viagra,free | **Spam** ) \n",
        "      = P( viagra | **Spam** ) * P( free | **Spam** )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLnNaeVwMPQ5"
      },
      "outputs": [],
      "source": [
        "from nltk import NaiveBayesClassifier\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(doc_features_set)\n",
        "\n",
        "train_set = doc_features_set[:350] # Since the total is 500\n",
        "test_set  = doc_features_set[150:]\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVCysILyR3LC"
      },
      "outputs": [],
      "source": [
        "# Most informative features calculated as:\n",
        "#  p(has(word)| class1 ) / p(has(word)| class2)\n",
        "classifier.show_most_informative_features(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQAQk60gMPQ5"
      },
      "source": [
        "## 5. Testing classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmSSn8c5MPQ5"
      },
      "outputs": [],
      "source": [
        "from nltk.classify import accuracy\n",
        "\n",
        "# Test on the entire test set\n",
        "print(accuracy(classifier, test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA8bPCpqMPQ6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 'ca01' is under the 'news' category\n",
        "classifier.classify(document_features(brown.words('ca01')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6-R9LfaMPQ6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# The test text needs to be long enough in order to contain a significant amount of the 1500 most common words in our training corpus.\n",
        "text = \"1 God, infinitely perfect and blessed in himself, in a plan of sheer goodness freely created man to make him share in his own blessed life. For this reason, at every time and in every place, God draws close to man. He calls man to seek him, to know him, to love him with all his strength. He calls together all men, scattered and divided by sin, into the unity of his family, the Church. To accomplish this, when the fullness of time had come, God sent his Son as Redeemer and Saviour. In his Son and through him, he invites men to become, in the Holy Spirit, his adopted children and thus heirs of his blessed life. 2 So that this call should resound throughout the world, Christ sent forth the apostles he had chosen, commissioning them to proclaim the gospel: \\\"Go therefore and make disciples of all nations, baptizing them in the name of the Father and of the Son and of the Holy Spirit, teaching them to observe all that I have commanded you; and lo, I am with you always, to the close of the age.\\\"4 Strengthened by this mission, the apostles \\\"went forth and preached everywhere, while the Lord worked with them and confirmed the message by the signs that attended it.\\\" 3 Those who with God's help have welcomed Christ's call and freely responded to it are urged on by love of Christ to proclaim the Good News everywhere in the world. This treasure, received from the apostles, has been faithfully guarded by their successors. All Christ's faithful are called to hand it on from generation to generation, by professing the faith, by living it in fraternal sharing, and by celebrating it in liturgy and prayer. 4 Quite early on, the name catechesis was given to the totality of the Church's efforts to make disciples, to help men believe that Jesus is the Son of God so that believing they might have life in his name, and to educate and instruct them in this life, thus building up the body of Christ. Catechesis is an education in the faith of children, young people and adults which includes especially the teaching of Christian doctrine imparted, generally speaking, in an organic and systematic way, with a view to initiating the hearers into the fullness of Christian life. While not being formally identified with them, catechesis is built on a certain number of elements of the Church's pastoral mission which have a catechetical aspect, that prepare for catechesis, or spring from it. They are: the initial proclamation of the Gospel or missionary preaching to arouse faith; examination of the reasons for belief; experience of Christian living; celebration of the sacraments; integration into the ecclesial community; and apostolic and missionary witness. Catechesis is intimately bound up with the whole of the Church's life. Not only her geographical extension and numerical increase, but even more her inner growth and correspondence with God's plan depend essentially on catechesis. Periods of renewal in the Church are also intense moments of catechesis. In the great era of the Fathers of the Church, saintly bishops devoted an important part of their ministry to catechesis. St. Cyril of Jerusalem and St. John Chrysostom, St. Ambrose and St. Augustine, and many other Fathers wrote catechetical works that remain models for us. The ministry of catechesis draws ever fresh energy from the councils. the Council of Trent is a noteworthy example of this. It gave catechesis priority in its constitutions and decrees. It lies at the origin of the Roman Catechism, which is also known by the name of that council and which is a work of the first rank as a summary of Christian teaching. The Council of Trent initiated a remarkable organization of the Church's catechesis. Thanks to the work of holy bishops and theologians such as St. Peter Canisius, St. Charles Borromeo, St. Turibius of Mongrovejo or St. Robert Bellarmine, it occasioned the publication of numerous catechisms. It is therefore no surprise that catechesis in the Church has again attracted attention in the wake of the Second Vatican Council, which Pope Paul Vl considered the great catechism of modern times. the General Catechetical Directory (1971) the sessions of the Synod of Bishops devoted to evangelization (1974) and catechesis (1977), the apostolic exhortations Evangelii nuntiandi (1975) and Catechesi tradendae (1979), attest to this. the Extraordinary Synod of Bishops in 1985 asked that a catechism or compendium of all Catholic doctrine regarding both faith and morals be composed. The Holy Father, Pope John Paul II, made the Synod's wish his own, acknowledging that this desire wholly corresponds to a real need of the universal Church and of the particular Churches. He set in motion everything needed to carry out the Synod Fathers' wish.\"\n",
        "text = \"Wang entered the address for the game into the browser. It had been easy to memorize: www.3body.net. The site indicated that the game only supported access via V-suit. Wang remembered that the employee lounge at the Nanotechnology Research Center had a V-suit. He left the now-empty main lab and went to the security office to get the key. In the lounge, he passed the pool tables and the exercise machines and found the V-suit next to a computer. He struggled into the haptic feedback suit, put on the panoramic viewing helmet, and turned on the computer.\"\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+') # Picks out sequences of alphanumeric characters as tokens and drops everything else\n",
        "text_tokens = nonstop(tokenizer.tokenize(text.lower()))\n",
        "text_tokens = [w for w in text_tokens if w.isalpha()]\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qDor1raMPQ-"
      },
      "outputs": [],
      "source": [
        "text_features = document_features(text_tokens)\n",
        "text_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXQjWlGLMPQ-"
      },
      "outputs": [],
      "source": [
        "classifier.classify(document_features(text_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4ccmRvbGkeV"
      },
      "outputs": [],
      "source": [
        "# A. PRESS: Reportage (44 texts)\n",
        "# B. PRESS: Editorial (27 texts)\n",
        "# C. PRESS: Reviews (17 texts)\n",
        "# D. RELIGION (17 texts)\n",
        "# E. SKILL AND HOBBIES (36 texts)\n",
        "# F. POPULAR LORE (48 texts)\n",
        "# G. BELLES-LETTRES - Biography, Memoirs, etc. (75 texts)\n",
        "# H. MISCELLANEOUS: US Government & House Organs (30 texts)\n",
        "# J. LEARNED - Natural sciences, Medicine, Mathematics, etc. (80 texts)\n",
        "# K. FICTION: General (29 texts)\n",
        "# L. FICTION: Mystery and Detective Fiction (24 texts)\n",
        "# M. FICTION: Science (6 texts)\n",
        "# N. FICTION: Adventure and Western (29 texts)\n",
        "# P. FICTION: Romance and Love Story (29 texts)\n",
        "# R. HUMOR (9 texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sI6-o6kXPLA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zY3QXoiXRPQ"
      },
      "source": [
        "# 1.6 **Bonus: Some more corpora in NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPdaQG0QXRPQ"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import inaugural\n",
        "nltk.download('inaugural')\n",
        "inaugural.fileids()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwJ-UCP8XRPR"
      },
      "outputs": [],
      "source": [
        "[fileid[:4] for fileid in inaugural.fileids()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCR2hzLjXRPR"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "           (target, fileid[:4])\n",
        "           for fileid in inaugural.fileids()\n",
        "           for w in inaugural.words(fileid)\n",
        "           for target in ['america', 'citizen']\n",
        "           if w.lower().startswith(target)) [1]\n",
        "cfd.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ0B6Rc8Xa4W"
      },
      "source": [
        "<img src=\"https://www.nltk.org/images/inaugural2.png\" alt=\"seq2seq\" style=\"width: 60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDxs9pFBXRPR"
      },
      "outputs": [],
      "source": [
        "nltk.download('cess_esp')\n",
        "# corpuses in other languages\n",
        "nltk.corpus.cess_esp.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqHKhFMvXRPS"
      },
      "outputs": [],
      "source": [
        "nltk.download('floresta')\n",
        "nltk.corpus.floresta.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fESXHR-ZXRPS"
      },
      "outputs": [],
      "source": [
        "nltk.download('indian')\n",
        "nltk.corpus.indian.words('hindi.pos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JeTpEmjXRPS"
      },
      "outputs": [],
      "source": [
        "nltk.download('udhr')\n",
        "nltk.corpus.udhr.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88WzS5EqXRPS"
      },
      "outputs": [],
      "source": [
        "nltk.download('udhr')\n",
        "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc13E50LQX1J"
      },
      "source": [
        "# 2.1 **Deriving N-Grams from Text:** *Creating n-grams (for language classification)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3kbY0I8QX1L"
      },
      "source": [
        "Based on [N-Gram-Based Text Categorization: Categorizing Text With Python by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/)\n",
        "\n",
        "What are n-grams? See [here](http://cloudmark.github.io/Language-Detection/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McTY8-riQX1M"
      },
      "source": [
        "## 1. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOWg5h0LQX1M"
      },
      "outputs": [],
      "source": [
        "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
        "s = s.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIu23K7FQX1N"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
        "s_tokenized = tokenizer.tokenize(s)\n",
        "s_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94fHsCzZQX1O"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "generated_4grams = []\n",
        "\n",
        "for word in s_tokenized:\n",
        "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))) # n = 4.\n",
        "generated_4grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeT9eu8-QX1O"
      },
      "source": [
        "It seems that `generated_4grams` needs flattening since it's supposed to be a list of 4-grams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pPJnGSRQX1O"
      },
      "outputs": [],
      "source": [
        "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
        "generated_4grams[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxvAlD3QX1P"
      },
      "source": [
        "## 2. Obtaining n-grams (n = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19GBVXnqQX1P"
      },
      "outputs": [],
      "source": [
        "ng_list_4grams = generated_4grams\n",
        "for idx, val in enumerate(generated_4grams):\n",
        "    ng_list_4grams[idx] = ''.join(val)\n",
        "ng_list_4grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMuK5ZwMQX1P"
      },
      "source": [
        "## 3. Sorting n-grams by frequency (n = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVpK1_ylQX1Q"
      },
      "outputs": [],
      "source": [
        "freq_4grams = {}\n",
        "\n",
        "for ngram in ng_list_4grams:\n",
        "    if ngram not in freq_4grams:\n",
        "        freq_4grams.update({ngram: 1})\n",
        "    else:\n",
        "        ngram_occurrences = freq_4grams[ngram]\n",
        "        freq_4grams.update({ngram: ngram_occurrences + 1})\n",
        "        \n",
        "from operator import itemgetter # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example, operator.add(x, y) is equivalent to the expression x + y.\n",
        "\n",
        "freq_4grams_sorted = sorted(freq_4grams.items(), key=itemgetter(1), reverse=True)[0:300] # We only keep the 300 most popular n-grams. This was suggested in the original paper written about n-grams.\n",
        "freq_4grams_sorted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIcousJ5QX1Q"
      },
      "source": [
        "## 4. Obtaining n-grams for multiple values of n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niX1PVkRQX1Q"
      },
      "source": [
        "To get n-grams for n = 1, 2, 3 and 4 we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrYQR517QX1R"
      },
      "outputs": [],
      "source": [
        "from nltk import everygrams\n",
        "\n",
        "s_clean = ' '.join(s_tokenized) # For the code below we need the raw sentence as opposed to the tokens.\n",
        "s_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5AYmf8qQX1R"
      },
      "outputs": [],
      "source": [
        "def ngram_extractor(sent):\n",
        "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4) \n",
        "            if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
        "\n",
        "ngram_extractor(s_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9QJBwbgRV_W"
      },
      "source": [
        "# 2.2 **Detecting Text Language by Counting Stop Words:** *A simple way to find out what language a text is written in*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTLs43kRV_b"
      },
      "source": [
        "Based on [Detecting Text Language With Python and NLTK by Alejandro Nolla](http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/)\n",
        "\n",
        "*Stop words* are words which are filtered out before processing because they are mostly grammatical as opposed to semantic in nature e.g. search engines remove words like 'want'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZznCl5mtRV_c"
      },
      "source": [
        "## 1. Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nimAvcmRV_c"
      },
      "outputs": [],
      "source": [
        "text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTyRAzgeRV_d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "try:\n",
        "    from nltk.tokenize import wordpunct_tokenize # RE-based tokenizer which splits text on whitespace and punctuation (except for underscore)\n",
        "except ImportError:\n",
        "    print('[!] You need to install nltk (http://nltk.org/index.html)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcatmhTkRV_e"
      },
      "outputs": [],
      "source": [
        "test_tokens = wordpunct_tokenize(text)\n",
        "test_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_0iyDWqRV_f"
      },
      "source": [
        "There are other tokenizers e.g. `RegexpTokenizer` where you can enter your own regexp, `WhitespaceTokenizer` (similar to Python's `string.split()`) and `BlanklineTokenizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYh_Hxw7RV_g"
      },
      "source": [
        "## 2. Exploring NLTK's stop words corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdPjsqNtRV_h"
      },
      "source": [
        "NLTK comes with a corpus of stop words in various languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIkmmNX7RV_h"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.readme().replace('\\n', ' ') # Since this is raw text, we need to replace \\n's with spaces for it to be readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_R5Kq5CRV_i"
      },
      "outputs": [],
      "source": [
        "stopwords.fileids() # Most corpora consist of a set of files, each containing a piece of text. A list of identifiers for these files is accessed via fileids()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9QOHkzfRV_i"
      },
      "source": [
        "Corpus readers provide a variety of methods to read data from the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKE3pmcgRV_j"
      },
      "outputs": [],
      "source": [
        "stopwords.raw('greek')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXJUECjZRV_j"
      },
      "outputs": [],
      "source": [
        "stopwords.raw('greek').replace('\\n', ' ') # Better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyzl2Gc2RV_j"
      },
      "outputs": [],
      "source": [
        "stopwords.words('english')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA3m1DlSRV_k"
      },
      "source": [
        "We can also use `.sents()` which returns sentences. However, in our particular case, this will cause an error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jODQ2TayRV_k"
      },
      "outputs": [],
      "source": [
        "#stopwords.sents('greek')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fKeL0ZTRV_k"
      },
      "source": [
        "The erro is because the `stopwords` corpus reader is of type `WordListCorpusReader` so there are no sentences.\n",
        "It's the same for `.paras()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbxF-68aRV_l"
      },
      "outputs": [],
      "source": [
        "len(stopwords.words(['english', 'greek'])) # There is a total of 444 Greek and English stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4C4oXjfRV_l"
      },
      "source": [
        "## 3. The classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXhpIIARV_l"
      },
      "source": [
        "We loop through the list of stop words in all languages and check how many stop words our test text contains in each language. The text is then classified to be in the language in which it has the most stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWP145X7RV_l"
      },
      "outputs": [],
      "source": [
        "language_ratios = {}\n",
        "\n",
        "test_words = [word.lower() for word in test_tokens] # lowercase all tokens\n",
        "test_words_set = set(test_words)\n",
        "\n",
        "for language in stopwords.fileids():\n",
        "    stopwords_set = set(stopwords.words(language)) # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n",
        "    common_elements = test_words_set.intersection(stopwords_set)\n",
        "    language_ratios[language] = len(common_elements) # language \"score\"\n",
        "    \n",
        "language_ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1Q82ETfRV_m"
      },
      "outputs": [],
      "source": [
        "most_rated_language = max(language_ratios, key=language_ratios.get) # The key parameter to the max() function is a function that computes a key. In our case, we already have a key so we set key to languages_ratios.get which actually returns the key.\n",
        "most_rated_language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc0FRzmkRV_n"
      },
      "outputs": [],
      "source": [
        "test_words_set.intersection(set(stopwords.words(most_rated_language))) # We can see which English stop words were found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lHYXUExWjJn"
      },
      "source": [
        "## 4. Plotting and Tabulating Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdOjq-6NWoui"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import udhr\n",
        "languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "          (lang, len(word))\n",
        "           for lang in languages\n",
        "           for word in udhr.words(lang + '-Latin1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_opF9sK0Wx3u"
      },
      "outputs": [],
      "source": [
        "cfd.tabulate(conditions=['English', 'German_Deutsch'],  samples=range(10), cumulative=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlLvRGz2R6fv"
      },
      "source": [
        "# 2.3 **Language Identifier Using Word Bigrams:** *State-of-the-art language classifier*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hvqGqbvR6fx"
      },
      "source": [
        "Based on [Language Identifier by asif31iqbal](https://github.com/asif31iqbal/language-identifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wEJo2zxR6fx"
      },
      "source": [
        "## 0. Importing libraries and creating helper tokenize method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6K1Cq8-Svx5"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/asif31iqbal/language-identifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVGDhldxR6fx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import string\n",
        "import os\n",
        "from nltk import ngrams, FreqDist, word_tokenize\n",
        "from numpy import arange\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nF-2x5GR6fy"
      },
      "outputs": [],
      "source": [
        "def ultimate_tokenize(sentence):\n",
        "    # Remove punctuation and digits\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
        "    return word_tokenize(sentence.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZuwBtyRR6fz"
      },
      "source": [
        "## 1. Understanding the process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjGIlU2YR6fz"
      },
      "outputs": [],
      "source": [
        "simple_example_text = 'Oh, then, I see Queen Mab hath been with you.'\n",
        "\n",
        "simple_example_tokens_words = ultimate_tokenize(simple_example_text)\n",
        "simple_example_tokens_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWaSuUs5R6f0"
      },
      "outputs": [],
      "source": [
        "simple_example_tokens_chars = list(simple_example_tokens_words[0])\n",
        "simple_example_tokens_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_fPE8DQR6f1"
      },
      "outputs": [],
      "source": [
        "simple_example_tokens_words_unigrams = list(ngrams(simple_example_tokens_words, 1))\n",
        "simple_example_tokens_words_unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95FQev1_R6f1"
      },
      "outputs": [],
      "source": [
        "simple_example_tokens_words_bigrams = list(ngrams(simple_example_tokens_words, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_'))\n",
        "simple_example_tokens_words_bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5ds1taGR6f2"
      },
      "outputs": [],
      "source": [
        "fdist = FreqDist(simple_example_tokens_words_unigrams)\n",
        "fdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTYKlpkTR6f2"
      },
      "outputs": [],
      "source": [
        "unigram_dict = dict()\n",
        "for k, v in fdist.items():\n",
        "        unigram_dict[' '.join(k)] = v\n",
        "unigram_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odzrWghoR6f2"
      },
      "outputs": [],
      "source": [
        "file = '/content/language-identifier/hm3_files/LangId.train.English'\n",
        "with open(file, encoding='utf8') as f:\n",
        "        content = f.read().lower()\n",
        "content.replace('\\n', '')[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J8fNs7XR6f3"
      },
      "outputs": [],
      "source": [
        "with open('/content/language-identifier/hm3_files/English.unigram.pickle', 'rb') as handle:\n",
        "    unigram_english_dict = pickle.load(handle)\n",
        "unigram_english_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWYQkaRhR6f3"
      },
      "outputs": [],
      "source": [
        "with open('/content/language-identifier/hm3_files/English.bigram.pickle', 'rb') as handle:\n",
        "    bigram_english_dict = pickle.load(handle)\n",
        "bigram_english_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-5WVsJLR6f3"
      },
      "outputs": [],
      "source": [
        "bigram_english_dict.get('of the')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMIoeca2R6f4"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "english_unigram_freqs = sorted(unigram_english_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "english_unigram_freqs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBTva_MbR6f4"
      },
      "outputs": [],
      "source": [
        "labels, values = zip(*english_unigram_freqs[:10])\n",
        "indexes = arange(len(labels))\n",
        "width = 0.8 # width = 1 would give bars that overlap because they are too close.\n",
        "\n",
        "fig = plt.figure(figsize=(10,7))                                                               \n",
        "ax = fig.gca() # Get current axis\n",
        "rects = ax.bar(indexes, values, width)\n",
        "\n",
        "# Add title and axis labels\n",
        "fig.suptitle('Top 10 English word unigrams', fontsize=20)\n",
        "plt.xlabel('Word unigram', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Display value of each bar on bar\n",
        "for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.text(rect.get_x() + rect.get_width() / 2., 50 + height, '%d' % int(height), ha='center', va='bottom') # Can also add color and fontweight arguments.\n",
        "\n",
        "# Remove the default x-axis tick numbers and use tick numbers of your own choosing:\n",
        "ax.set_xticks(indexes)\n",
        "# Replace the tick numbers with strings:\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "plt.show()\n",
        "# plt.savefig('top10EnglishWordUnigrams.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCE1-n8sR6f4"
      },
      "source": [
        "## 1. Generating unigram and bigram frequencies for English, French and Italian from training files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMAL5GRlR6f5"
      },
      "outputs": [],
      "source": [
        "def get_ngram_count_dict(tokens, n):\n",
        "    if n == 1:\n",
        "        n_grams = ngrams(tokens, n)\n",
        "    else:\n",
        "        n_grams = ngrams(tokens, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_') # Fun fact: If I remove padding here and later when testing, and also remove the '_' from the unigram dicts, the accuracy rises slightly. However, it's not statistically significant due to the small size of the data.\n",
        "    fdist = FreqDist(n_grams)\n",
        "    ngram_dict = dict()\n",
        "    for k,v in fdist.items():\n",
        "        ngram_dict[' '.join(k)] = v\n",
        "    return ngram_dict\n",
        "\n",
        "# Calls get_ngram_count_dict to get a unigram and bigram dict from file.\n",
        "def get_unigram_bigram_dicts(file):\n",
        "    with open(file, encoding='utf8') as f:\n",
        "        content = f.read()\n",
        "    tokens = ultimate_tokenize(content)\n",
        "    unigram_dict = get_ngram_count_dict(tokens, 1)     \n",
        "    bigram_dict = get_ngram_count_dict(tokens, 2)     \n",
        "    return (unigram_dict, bigram_dict)\n",
        "\n",
        "# Dumps unigram and bigram dictionary of training data of given language to .pickle files.\n",
        "def dump_pickle(language):\n",
        "    file = '/content/language-identifier/hm3_files/LangId.train.' + language #+ '.txt'\n",
        "    unigram_dict, bigram_dict = get_unigram_bigram_dicts(file)\n",
        "    with open('/content/language-identifier/hm3_files/' + language + '.unigram.pickle', 'wb') as handle:\n",
        "        pickle.dump(unigram_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) # HIGHEST_PROTOCOL instructs pickle to use the highest protocol version available.\n",
        "    with open('/content/language-identifier/hm3_files/' + language + '.bigram.pickle', 'wb') as handle:\n",
        "        pickle.dump(bigram_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "dump_pickle('English')\n",
        "dump_pickle('French')\n",
        "dump_pickle('Italian')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inAF1AlAR6f5"
      },
      "source": [
        "Later, it will also be required to know how many sentences there are in the training data for each language. This is because of the method used to calculate probabilities (incorporating the probability of the bigram among other bigrams starting with the same word) and the fact we use padding for our bigrams. \n",
        "\n",
        "In our training data each line is a sentence, which is very convenient for calculating the number of sentences.\n",
        "\n",
        "We go ahead and get the number of sentences (for more efficiency, the following code could be added to `get_unigram_bigram_dicts`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZNXf06IR6f6"
      },
      "outputs": [],
      "source": [
        "with open('/content/language-identifier/hm3_files/LangId.train.English', encoding='utf8') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        pass\n",
        "number_of_sents_en = i + 1\n",
        "with open('/content/language-identifier/hm3_files/LangId.train.French', encoding='utf8') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        pass\n",
        "number_of_sents_fr = i + 1\n",
        "with open('/content/language-identifier/hm3_files/LangId.train.Italian', encoding='utf8') as f:\n",
        "    for i, l in enumerate(f):\n",
        "        pass\n",
        "number_of_sents_it = i + 1\n",
        "\n",
        "print('NUMBER OF SENTENCES IN TRAINING DATA')\n",
        "print('English:', number_of_sents_en)\n",
        "print('French:', number_of_sents_fr)\n",
        "print('Italian:', number_of_sents_it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGOvN4njR6f6"
      },
      "source": [
        "## 2. Identifying language for each line of the test file using bigram probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqviQvXuR6f7"
      },
      "outputs": [],
      "source": [
        "with open('/content/language-identifier/hm3_files/English.unigram.pickle', 'rb') as handle:\n",
        "    unigram_english_dict = pickle.load(handle)\n",
        "    \n",
        "with open('/content/language-identifier/hm3_files/English.bigram.pickle', 'rb') as handle:\n",
        "    bigram_english_dict = pickle.load(handle)\n",
        "    \n",
        "with open('/content/language-identifier/hm3_files/French.unigram.pickle', 'rb') as handle:\n",
        "    unigram_french_dict = pickle.load(handle)\n",
        "    \n",
        "with open('/content/language-identifier/hm3_files/French.bigram.pickle', 'rb') as handle:\n",
        "    bigram_french_dict = pickle.load(handle)\n",
        "    \n",
        "with open('/content/language-identifier/hm3_files/Italian.unigram.pickle', 'rb') as handle:\n",
        "    unigram_italian_dict = pickle.load(handle)\n",
        "    \n",
        "with open('/content/language-identifier/hm3_files/Italian.bigram.pickle', 'rb') as handle:\n",
        "    bigram_italian_dict = pickle.load(handle)\n",
        "    \n",
        "vocabulary_size = len(unigram_english_dict) + len(unigram_french_dict) + len(unigram_italian_dict)\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJyETO_jR6f7"
      },
      "outputs": [],
      "source": [
        "# Get probability of given bigram belonging to the language which bigram_dict is in\n",
        "def get_bigram_probability(bigram, first_word, bigram_dict, first_word_dict): # first_word is the first word of the word bigram.\n",
        "    bigram_count = bigram_dict.get(bigram)\n",
        "    if bigram_count is None:\n",
        "        bigram_count = 0\n",
        "    \n",
        "    first_word_count = first_word_dict.get(first_word)\n",
        "    if first_word_count is None:\n",
        "        first_word_count = 0\n",
        "    \n",
        "    return (bigram_count + 1) / (first_word_count + vocabulary_size) # To get the logic of this formula, note how the proability is used in the function below. Without the + 1 in the Nr, if you find a bigram which is not in our known bigrams for a language, the probability of it being in that language would become 0. So we would like to assign a small probability of 1 / vocabulary_size in that case. Also note the arbitrariness of this 'probability'. We're saying \"Given a bigram and a language, what is the probability that the bigram is of that language?\" This is arbitrary because to get a meaningful probability we need to know which are the other languages considered and what their bigram frequencies are. That would be another way to do it, but arguable a worse one because it wouldn't be able to give a confidence score for a particular language. The formula just uses common sense to get to a number which works for the purposes. In the denominator, we have both first_word_count and vocabulary_size. Why? We have vocabulary_size for all langs in the denom because the larger this is, the less significant it is that for this particular language the bigram appears so many times. Could we have used a vocab_size of bigrams instead of unigrams? Sure, and the 'probabilities' would end up being much smaller numbers. What about first_word_count? This gives us a way to compare this bigram against other bigrams in this language starting with the same word. In general though, for a given bigram, it's more important to consider how many times it exists than to consider whether it is the usual bigram given a certain first word. The formula achieves that. Take the bigram 'le monseiur' and the English language. Let's say the bigram appears once and 'le' also appears once, while in French 'le monseiur' appears 100 times and le appears 100,000 times. Probability for English = (1 + 1) / (1 + 20,000) = 0.000099995. Probability for French = (100 + 1) / (100,000 + 20,000) = 0.00084166666. Note how the probability for French is still low because 100/100,000 is quite low and maybe it's not French after all if in French le is usually followed by other words. However, it's still significantly higher than the probability for English where both 'le' and 'le monseiur' only appear once.\n",
        "\n",
        "# Get probability that a given bigram list is of a language (specified by its bigram_dict)\n",
        "def get_language_probability(bigram_list, first_words, bigram_dict, first_word_dict):\n",
        "    result = 1.0\n",
        "    index = 0\n",
        "    for bigram in bigram_list:\n",
        "        result *= get_bigram_probability(bigram, first_words[index], bigram_dict, first_word_dict)\n",
        "        index += 1\n",
        "    return result\n",
        "\n",
        "# Load correct solutions\n",
        "solution_dict = dict()\n",
        "with open('/content/language-identifier/hm3_files/LangId.sol') as f:\n",
        "    for line in f:\n",
        "       (key, val) = line.split()\n",
        "       solution_dict[int(key)] = val\n",
        "        \n",
        "line_no = 1\n",
        "result_dict = dict()\n",
        "correct = 0\n",
        "incorrect_line_numbers = []\n",
        "\n",
        "# This needs to be done because I'm using padding for bigrams so the unigram dicts in their raw forms can't be used in get_bigram_probability():\n",
        "unigram_english_dict['_'] = number_of_sents_en\n",
        "unigram_french_dict['_'] = number_of_sents_fr\n",
        "unigram_italian_dict['_'] = number_of_sents_it\n",
        "\n",
        "with open('/content/language-identifier/hm3_files/LangId.test', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        tokens = ultimate_tokenize(line)\n",
        "        bigrams = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')\n",
        "        bigram_list = [] # bigram_list will be exactly like bigrams but instead of [('_', 'this'), ...] it will be ['_ this', ...]. It is required because this is how bigrams are represented in the dictionary.\n",
        "        first_words = [] # The first words of each bigram. This is the similar to making a unigram_list. We use it because we don't want something in the form [(this,), ...]. Also because we want this to include '_'. We want it to include '_' because we're not using the unigrams for classification but as part of a formula to judge bigram frequency based on the starting word.\n",
        "        for b in bigrams:\n",
        "            bigram_list.append(' '.join(b))\n",
        "            first_words.append(b[0])\n",
        "        \n",
        "        english_prob = get_language_probability(bigram_list, first_words, bigram_english_dict, unigram_english_dict)\n",
        "        french_prob = get_language_probability(bigram_list, first_words, bigram_french_dict, unigram_french_dict)\n",
        "        italian_prob = get_language_probability(bigram_list, first_words, bigram_italian_dict, unigram_italian_dict)\n",
        "        \n",
        "        max_prob = max(english_prob, french_prob, italian_prob)\n",
        "        if max_prob == english_prob:\n",
        "            result_dict[line_no] = 'English'\n",
        "        elif max_prob == french_prob:\n",
        "            result_dict[line_no] = 'French'\n",
        "        else:\n",
        "            result_dict[line_no] = 'Italian'\n",
        "        \n",
        "        if solution_dict[line_no] == result_dict[line_no]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect_line_numbers.append(line_no)\n",
        "            \n",
        "        line_no += 1\n",
        "\n",
        "# Storing results from result_dict to file:\n",
        "with open('/content/language-identifier/hm3_files/LangId.result', 'w') as f:\n",
        "    for (key, val) in result_dict.items():\n",
        "        f.write(' '.join([str(key), val]) + '\\n')\n",
        "        \n",
        "print('Accuracy: {:2.2f}%'.format(correct * 100 / len(solution_dict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voaPi-nFR6f9"
      },
      "outputs": [],
      "source": [
        "print('Line numbers for incorrectly classified languages: {}'.format(str(incorrect_line_numbers)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UlknopXR6f-"
      },
      "source": [
        "## 3. Testing with our own sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkMxVXUsR6f_"
      },
      "outputs": [],
      "source": [
        "sent = \"This is a sentence.\"\n",
        "sent_tokens = ultimate_tokenize(sent)\n",
        "sent_bigrams_pre = ngrams(sent_tokens, 2, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')\n",
        "sent_bigrams = []\n",
        "sent_bigrams_first_words = []\n",
        "for b in sent_bigrams_pre:\n",
        "    sent_bigrams.append(' '.join(b))\n",
        "    sent_bigrams_first_words.append(b[0])\n",
        "print('Sentence bigrams:', sent_bigrams)\n",
        "print('Sentence bigrams first words:', sent_bigrams_first_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtig6ngxR6f_"
      },
      "outputs": [],
      "source": [
        "sent_english_prob = get_language_probability(sent_bigrams, sent_bigrams_first_words, bigram_english_dict, unigram_english_dict)\n",
        "sent_french_prob = get_language_probability(sent_bigrams, sent_bigrams_first_words, bigram_french_dict, unigram_french_dict)\n",
        "sent_italian_prob = get_language_probability(sent_bigrams, sent_bigrams_first_words, bigram_italian_dict, unigram_italian_dict)\n",
        "print(\"RAW 'PROBABILITIES'\")\n",
        "print('English:', sent_english_prob)\n",
        "print('French:', sent_french_prob)\n",
        "print('Italian:', sent_italian_prob)\n",
        "# As we can see, these 'probabilities' are arbitrary. We can try to convert them to percentages since we are classifying only among these 3 languages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYIZWA6bR6f_"
      },
      "outputs": [],
      "source": [
        "def get_normalized_probabilities(list_of_probabilities):\n",
        "    sum_of_probabilities = sum(list_of_probabilities)\n",
        "    result = []\n",
        "    for probability in list_of_probabilities:\n",
        "        result.append(probability / sum_of_probabilities)\n",
        "    return result\n",
        "\n",
        "probabilities = [sent_english_prob, sent_french_prob, sent_italian_prob]\n",
        "normalized_probabilities = get_normalized_probabilities(probabilities)\n",
        "\n",
        "print('RELATIVE PROBABILITIES')\n",
        "print('English: ', round(normalized_probabilities[0] * 100, 2), '%', sep='') # I use sep because I don't want a space before the % sign.\n",
        "print('French: ', round(normalized_probabilities[1] * 100, 2), '%', sep='')\n",
        "print('Italian: ', round(normalized_probabilities[2] * 100, 2), '%', sep='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyuBq19oR6f_"
      },
      "source": [
        "**PS:** For a state-of-the-art Greek dialect classifier using n-grams, take a look at [Greek Dialect Classifier](https://github.com/hb20007/greek-dialect-classifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcnFBbIsUcsk"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import inaugural\n",
        "nltk.download('inaugural')\n",
        "inaugural.fileids()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV9aFN0HUgdv"
      },
      "outputs": [],
      "source": [
        "[fileid[:4] for fileid in inaugural.fileids()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2px9AaPUlHw"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "           (target, fileid[:4])\n",
        "           for fileid in inaugural.fileids()\n",
        "           for w in inaugural.words(fileid)\n",
        "           for target in ['america', 'citizen']\n",
        "           if w.lower().startswith(target)) [1]\n",
        "cfd.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVidPrxsU40H"
      },
      "outputs": [],
      "source": [
        "nltk.download('cess_esp')\n",
        "# corpuses in other languages\n",
        "nltk.corpus.cess_esp.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzC4XZsqU7hr"
      },
      "outputs": [],
      "source": [
        "nltk.download('floresta')\n",
        "nltk.corpus.floresta.words()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTI75Z-lVAWB"
      },
      "outputs": [],
      "source": [
        "nltk.download('indian')\n",
        "nltk.corpus.indian.words('hindi.pos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPhMPHYJVDEm"
      },
      "outputs": [],
      "source": [
        "nltk.download('udhr')\n",
        "nltk.corpus.udhr.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGvw7fApVHAb"
      },
      "outputs": [],
      "source": [
        "nltk.download('udhr')\n",
        "nltk.corpus.udhr.words('Javanese-Latin1')[11:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8PHo6jrGerm"
      },
      "source": [
        "# 3.1 **Bigrams, Stemming and Lemmatizing:** *NLTK makes bigrams, stemming and lemmatization super-easy*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQc-bG_-Gero"
      },
      "source": [
        "## 1. Exploring the `reuters` corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfywyYNCGero"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import reuters\n",
        "\n",
        "reuters.readme().replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B8dFlxhGerq"
      },
      "outputs": [],
      "source": [
        "reuters.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiL77GV8Gerq"
      },
      "outputs": [],
      "source": [
        "reuters.fileids()[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCGGKg3kGerr"
      },
      "outputs": [],
      "source": [
        "len(reuters.fileids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcH3F2fXTt-K"
      },
      "outputs": [],
      "source": [
        "reuters.fileids('barley')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2g4ulIuTv3P"
      },
      "outputs": [],
      "source": [
        "reuters.fileids(['barley', 'corn'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QYnHgMCGerr"
      },
      "outputs": [],
      "source": [
        "reuters.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbFfO9JOT1oq"
      },
      "outputs": [],
      "source": [
        "reuters.categories(['training/9865', 'training/9880'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFiMI8ACGers"
      },
      "outputs": [],
      "source": [
        "reuters.sents('test/14826')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxRKAti4T5iZ"
      },
      "outputs": [],
      "source": [
        "reuters.words(categories='barley')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7DqO2LzT7KD"
      },
      "outputs": [],
      "source": [
        "reuters.words(categories=['barley', 'corn'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0erApEzoGers"
      },
      "source": [
        "## 2. Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77C4vvCJGert"
      },
      "outputs": [],
      "source": [
        "trade_words = reuters.words(categories='trade')\n",
        "len(trade_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjrzzbQKGeru"
      },
      "outputs": [],
      "source": [
        "trade_words_condensed = trade_words[:100]\n",
        "trade_words_condensed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0p-NTwCGeru"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Remove stopwords from trade_words_condensed and lower case it\n",
        "trade_words_condensed = [w.lower() for w in trade_words_condensed if w.lower() not in stopwords.words('english')]\n",
        "trade_words_condensed[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Naa6HPXEGeru"
      },
      "outputs": [],
      "source": [
        "import string # Contains string constants eg. ascii_lowercase which is 'a...z', string formatting functions, other string functions like .capwords() and .translate().\n",
        "\n",
        "# Remove punctuation\n",
        "# trade_words_condensed = [w for w in trade_words_condensed if w not in string.punctuation]\n",
        "punct_combo = [c + \"\\\"\" for c in string.punctuation ] + [\"\\\"\" + c for c in string.punctuation] + [\".-\", \":-\", \"..\", \"...\"]\n",
        "trade_words_condensed = [w for w in trade_words_condensed if w not in string.punctuation and w not in punct_combo]\n",
        "trade_words_condensed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn-PdTEeGerv"
      },
      "outputs": [],
      "source": [
        "from nltk import bigrams\n",
        "\n",
        "bi_trade_words_condensed = list(bigrams(trade_words_condensed))\n",
        "bi_trade_words_condensed[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gSoOghnGerv"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "bi_fdist = FreqDist(bi_trade_words_condensed)\n",
        "\n",
        "for word, frequency in bi_fdist.most_common(3):\n",
        "    print(word, frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHzj5lnWGerv"
      },
      "outputs": [],
      "source": [
        "bi_fdist.plot(3, cumulative=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Uqs1CX-Gerw"
      },
      "source": [
        "## 3. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE2Bel7MGerw"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
        "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "print(porter.stem('Re-testing'), lancaster.stem('Re-testing'), snowball.stem('Re-testing'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGMGYvBJGerw"
      },
      "outputs": [],
      "source": [
        "# Fun fact: SnowballStemmer can stem several other languages beside English.\n",
        "# To make, for instance, a French stemmer, we can do the following: french_stemmer = SnowballStemmer('french')\n",
        "SnowballStemmer.languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKgtX-aoGerw"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"So, we'll go no more a-roving. So late into the night, Though the heart be still as loving, And the moon be still as bright.\"\n",
        "\n",
        "# This uses the 3-argument version of str.maketrans with arguments (x, y, z) where 'x' and 'y' must be equal-length strings and characters in 'x' are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character in the string is mapped to None\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "translator\n",
        "\n",
        "# This is an alternative that creates a dictionary mapping of every character from string.punctuation to None (this will also work but creates a whole dictionary so is slower)\n",
        "#translator = str.maketrans(dict.fromkeys(string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmKuY34CGerx"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize(sentence.translate(translator))\n",
        "tokens[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogE4yixFGerx"
      },
      "outputs": [],
      "source": [
        "for stemmer in [porter, lancaster, snowball]:\n",
        "    print([stemmer.stem(t) for t in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZgtgFUkGerx"
      },
      "source": [
        "## 4. Lemmatizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKcPMOSNGerx"
      },
      "source": [
        "Lemmatization aims to achieve a similar base \"stem\" for a word, but aims to derive the genuine dictionary root word, not just a trunctated version of the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI_r-xR-Cq32"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBSe7tjYGery"
      },
      "outputs": [],
      "source": [
        "# The default lemmatization method with the Python NLTK is the WordNet lemmatizer.\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(wnl.lemmatize('brightening'), wnl.lemmatize('boxes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj6nzJBfGery"
      },
      "outputs": [],
      "source": [
        "# As we saw above, sometimes, if we try to lemmatize a word, it will end up with the same word. This is because the default part of speech is nouns.\n",
        "wnl.lemmatize('brightening', pos='v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9efdXT1HX-nl"
      },
      "source": [
        "## 5. Generating Random Text with Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8zx0ZRaYdZg"
      },
      "outputs": [],
      "source": [
        "# We can use a conditional frequency distribution to create a table of bigrams (word pairs).\n",
        "# The bigrams() function takes a list of words and builds a list of consecutive word pairs. \n",
        "# Remember that, in order to see the result and not a cryptic \"generator object\", we need to use the list() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYVywqgKYQ9L"
      },
      "outputs": [],
      "source": [
        "text = nltk.corpus.genesis.words('english-kjv.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZLzA5fHaBta"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1tvKwtcZ_tI"
      },
      "outputs": [],
      "source": [
        "bigrams = nltk.bigrams(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoq51FStaDoe"
      },
      "outputs": [],
      "source": [
        "bigrams_disp = list( nltk.bigrams(text))\n",
        "print(bigrams_disp[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDXVnh_caAqO"
      },
      "outputs": [],
      "source": [
        "cfd = nltk.ConditionalFreqDist(bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgxko-gbac6O"
      },
      "outputs": [],
      "source": [
        "cfd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FjlhjqnYIRk"
      },
      "outputs": [],
      "source": [
        "nltk.download('genesis')\n",
        "\n",
        "# constructs a conditional frequency distribution to record which words are most likely to follow a given word\n",
        "def generate_model(cfdist, word, num=15):\n",
        "    for i in range(num):\n",
        "        print(word, end=' ')\n",
        "        word = cfdist[word].max()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FHlx-6xYPmc"
      },
      "outputs": [],
      "source": [
        "cfd['living']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMUyMd6zYSfX"
      },
      "outputs": [],
      "source": [
        "generate_model(cfd, 'creature')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCrdQjm7Yz6W"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O shakespeare.txt\n",
        "shakespeare = open(\"shakespeare.txt\").readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kfGFtkMYaSj"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "sbigrams = nltk.bigrams(nltk.word_tokenize(' '.join(shakespeare)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNgTnWu7ZHH0"
      },
      "outputs": [],
      "source": [
        "scfd = nltk.ConditionalFreqDist(sbigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jyTgNJzZKB2"
      },
      "outputs": [],
      "source": [
        "scfd['thou']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBGrQq6hZfOi"
      },
      "outputs": [],
      "source": [
        "generate_model(scfd, 'terrible')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI6vgF5bas7V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Can you make the generator more diverse?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-LSxRdrbNhb"
      },
      "outputs": [],
      "source": [
        "better_generator('thou', scfd, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDbw99-hcX1s"
      },
      "outputs": [],
      "source": [
        "scfd['?']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjbHaUqvHJtj"
      },
      "source": [
        "# 3.2 **Finding Unusual Words in Given Language:** *Which words do not belong with the rest of the text?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LR2XlKqHJtl"
      },
      "outputs": [],
      "source": [
        "text = \"Truly Kryptic is the best puzzle game. It's browser-based and free. Google it.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2VW183gHJtl"
      },
      "source": [
        "## 1. Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwCic_XKHJtm"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "text_tokenized = word_tokenize(text.lower())\n",
        "text_tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEya_lxxHJtn"
      },
      "source": [
        "## 2. Importing and exploring the words corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAeRZtdCHJtn"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import words\n",
        "nltk.download('words')\n",
        "words.readme().replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZteqHmbHJtn"
      },
      "outputs": [],
      "source": [
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADWWCsZzHJto"
      },
      "outputs": [],
      "source": [
        "words.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT7GHasJHJto"
      },
      "outputs": [],
      "source": [
        "words.words('en')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viPBk2-1HJto"
      },
      "outputs": [],
      "source": [
        "words.words('en-basic')[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_lFRdmZHJtp"
      },
      "outputs": [],
      "source": [
        "len(words.words('en'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qphzxMXxHJtp"
      },
      "outputs": [],
      "source": [
        "len(words.words('en-basic'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WOKTL5gg-hu"
      },
      "source": [
        "## 3. Finding unusual words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX_o1Dtsg-hu"
      },
      "outputs": [],
      "source": [
        "english_vocab = set(w.lower() for w in words.words())\n",
        "text_vocab = set(w.lower() for w in text_tokenized if w.isalpha()) # Note .isalpha() removes punctuation tokens. However, tokens with a hyphen like 'browser-based' are totally skipped over because .isalpha() would be false.\n",
        "unusual = text_vocab.difference(english_vocab)\n",
        "unusual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5L-LgR6HhN_"
      },
      "source": [
        "We can train a classifier to work out which suffixes are most informative for POS tagging. We can begin by finding out what the most common suffixes are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u0VS5__HhN_"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "from nltk import FreqDist\n",
        "\n",
        "suffix_fdist = FreqDist()\n",
        "for word in brown.words():\n",
        "    word = word.lower()\n",
        "    suffix_fdist[word[-1:]] += 1\n",
        "    suffix_fdist[word[-2:]] += 1\n",
        "    suffix_fdist[word[-3:]] += 1\n",
        "    \n",
        "suffix_fdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgHagU04HhOA"
      },
      "outputs": [],
      "source": [
        "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
        "common_suffixes[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Udg999HhOB"
      },
      "source": [
        "Next, we'll define a feature extractor function which checks a given word for these suffixes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYCylkYpHhOB"
      },
      "outputs": [],
      "source": [
        "def pos_features(word):\n",
        "    features = {}\n",
        "    for suffix in common_suffixes:\n",
        "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
        "    return features\n",
        "\n",
        "pos_features('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBxQiIrHhOB"
      },
      "source": [
        "Now that we've defined our feature extractor, we can use it to train a new decision tree classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQOBk0BkHhOC"
      },
      "outputs": [],
      "source": [
        "tagged_words = brown.tagged_words(categories='news')\n",
        "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
        "featuresets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCB7KRyy0G16"
      },
      "source": [
        "**Decision trees**\n",
        "\n",
        "\n",
        "*   **Node**: Represents each feature\n",
        "\n",
        "*   **Leaf nodes**: Represents labels\n",
        "\n",
        "*   **Branches**: represent conjunctions of features that lead to those class\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://forum.huawei.com/enterprise/en/data/attachment/forum/202103/24/190400o09x7rhnnhy2yon7.png?1.png\" alt=\"seq2seq\" style=\"width: 60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va5HhSpHHhOC"
      },
      "outputs": [],
      "source": [
        "from nltk import DecisionTreeClassifier\n",
        "from nltk.classify import accuracy\n",
        "\n",
        "cutoff = int(len(featuresets) * 0.1)\n",
        "train_set, test_set = featuresets[cutoff:], featuresets[:cutoff]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-264RBMHhOD"
      },
      "outputs": [],
      "source": [
        "#classifier = DecisionTreeClassifier.train(train_set) # NLTK is a teaching toolkit which is not really optimized for speed. Therefore, this may take forever. For speed, use scikit-learn for the classifiers.\n",
        "\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "#bayes_classifier = SklearnClassifier(BernoulliNB(), sparse=False).train(train_set)\n",
        "#svm_classifier = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
        "classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
        "#print(accuracy(classifier, test_set) , accuracy(bayes_classifier, test_set))\n",
        "#print(accuracy(classifier, test_set) , accuracy(svm_classifier, test_set), accuracy(bayes_classifier, test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yYF5NOTHhOD"
      },
      "outputs": [],
      "source": [
        "accuracy(classifier, test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JYvWifuHhOE"
      },
      "outputs": [],
      "source": [
        "classifier.classify(pos_features('cats'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XijkIZiHh56R"
      },
      "outputs": [],
      "source": [
        "classifier.classify(pos_features('cat'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HWKjNxKhL6y"
      },
      "outputs": [],
      "source": [
        "len(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgmDvd8bgdDZ"
      },
      "outputs": [],
      "source": [
        "# Use nltk decision tree\n",
        "# We should reduce the training data (90499 takes too long with nltk's implementation of decision tree)\n",
        "nltk_dt = nltk.DecisionTreeClassifier.train(train_set[:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8-BbpACiBKf"
      },
      "outputs": [],
      "source": [
        "accuracy(nltk_dt, test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQXkbZNlgjt_"
      },
      "outputs": [],
      "source": [
        "nltk_dt.classify(pos_features('cats'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzIo_B-qh9TY"
      },
      "outputs": [],
      "source": [
        "classifier.classify(pos_features('cat'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNghTDuIHhOE"
      },
      "outputs": [],
      "source": [
        "print(nltk_dt.pseudocode(depth=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXAgKYQ3HJtp"
      },
      "outputs": [],
      "source": [
        "english_vocab = set(w.lower() for w in words.words())\n",
        "text_vocab = set(w.lower() for w in text_tokenized if w.isalpha()) # Note .isalpha() removes punctuation tokens. However, tokens with a hyphen like 'browser-based' are totally skipped over because .isalpha() would be false.\n",
        "unusual = text_vocab.difference(english_vocab)\n",
        "unusual"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dhklFH83P_RZ",
        "YMuK5ZwMQX1P",
        "WIcousJ5QX1Q",
        "aYh_Hxw7RV_g",
        "PCE1-n8sR6f4",
        "xGOvN4njR6f6",
        "9Uqs1CX-Gerw"
      ],
      "name": "[Student] NLTK Part 1-a.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('2022_AI_expert-VdvU_stD')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "6f9b4e903e38bfd606aa85746352f0c68a834a97143e8a5ff660a0ae957bcf1b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
